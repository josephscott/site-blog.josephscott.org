{
    "version": "https://jsonfeed.org/version/1.1",
    "user_comment": "This feed allows you to read the posts from this site in any feed reader that supports the JSON Feed format. To add this feed to your reader, copy the following URL -- https://blog.josephscott.org/category/database/feed/json/ -- and add it your reader.",
    "home_page_url": "https://blog.josephscott.org/category/database/",
    "feed_url": "https://blog.josephscott.org/category/database/feed/json/",
    "language": "en-US",
    "title": "Database &#8211; Joseph Scott",
    "items": [
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/05/04/oracle-10g-vs-postgresql-8-vs-mysql-5/",
            "url": "https://blog.josephscott.org/2005/05/04/oracle-10g-vs-postgresql-8-vs-mysql-5/",
            "title": "Oracle 10g vs PostgreSQL 8 vs MySQL 5",
            "content_html": "<p>In contrast to my <a href=\"http://joseph.randomnetworks.com/archives/2005/05/03/benchmarks-not-enough-details/\">recent rant about database comparisons</a>, I was pretty impressed with the <a href=\"http://www.suite101.com/article.cfm/oracle/115560\">Oracle 10g vs PostgreSQL 8 vs MySQL 5</a> article.  The two articles don&#8217;t compare the same issues, but I&#8217;d like to point out how well this one done.</p>\n<p>Right from the start the <a href=\"http://www.suite101.com/profile.cfm/lewisc\">author</a> states his bias upfront (he is a long time Oracle admin), along with what was being compared.  In this case the focus was on installing on a lower end Windows 2000 system.  All of the factors were spelled out and the scoring method was described in sufficient detail.  Each factor had some discussion and and a score for each database.</p>\n<p>Overall I&#8217;d consider this a good example of a database comparison, it has a well defined (and narrow) focus with an upfront scoring system.  My only complaint with this review was in the &#8220;Documentation and Getting Started Support&#8221; section.  Although PostgreSQL and MySQL basically have the same issues in the discussion area, the scores are different.  Given how well the rest of the review was done I don&#8217;t think this issue completely ruins the review, just provides a low point.</p>\n<p>I hope that the author, <a href=\"http://blogs.ittoolbox.com/oracle/guide/\">Lewis R Cunningham</a>, continues to do additional reviews of these three databases using this same format.</p>\n",
            "content_text": "In contrast to my recent rant about database comparisons, I was pretty impressed with the Oracle 10g vs PostgreSQL 8 vs MySQL 5 article.  The two articles don&#8217;t compare the same issues, but I&#8217;d like to point out how well this one done.\nRight from the start the author states his bias upfront (he is a long time Oracle admin), along with what was being compared.  In this case the focus was on installing on a lower end Windows 2000 system.  All of the factors were spelled out and the scoring method was described in sufficient detail.  Each factor had some discussion and and a score for each database.\nOverall I&#8217;d consider this a good example of a database comparison, it has a well defined (and narrow) focus with an upfront scoring system.  My only complaint with this review was in the &#8220;Documentation and Getting Started Support&#8221; section.  Although PostgreSQL and MySQL basically have the same issues in the discussion area, the scores are different.  Given how well the rest of the review was done I don&#8217;t think this issue completely ruins the review, just provides a low point.\nI hope that the author, Lewis R Cunningham, continues to do additional reviews of these three databases using this same format.",
            "date_published": "2005-05-04T23:31:37-06:00",
            "date_modified": "2005-05-04T23:31:37-06:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "MySQL",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/05/03/benchmarks-never-enough-details/",
            "url": "https://blog.josephscott.org/2005/05/03/benchmarks-not-enough-details/",
            "title": "Benchmarks, Not Enough Details",
            "content_html": "<p>Once again there is another attempt at comparing <a href=\"http://www.mysql.com/\">MySQL</a> vs. <a href=\"http://www.postgresql.org/\">PostgreSQL</a>.  Like so many others before it, this benchmark falls prey to a classic mistake, not enough details.  So lets go through their review shall we?</p>\n<p><strong>Comparison</strong><br />\n<strong>Operating System</strong>: All of the OSs listed for MySQL also apply to PostgreSQL.  No explanation is given as to why they only listed two for PostgreSQL.  There are even <a href=\"http://www.postgresql.org/docs/faq/\">FAQ</a> entries for some OSs, like <a href=\"http://www.postgresql.org/docs/faqs.FAQ_AIX.html\">AIX</a>, <a href=\"http://www.postgresql.org/docs/faqs.FAQ_HPUX.html\">HPUX</a>, <a href=\"http://www.postgresql.org/docs/faqs.FAQ_IRIX.html\">IRIX</a> and <a href=\"http://www.postgresql.org/docs/faqs.FAQ_Solaris.html\">Solaris</a>.</p>\n<p><strong>Performance</strong>: I&#8217;ll go into more detail on this when later, suffice it say that this is a gross over simplification.</p>\n<p><strong>Other APIs</strong>: I&#8217;m not sure why they describe MySQL with &#8220;Most of languages&#8221; and mention specific languages for PostgreSQL.  I suspect that virtually any language that has MySQL support also has PostgreSQL support.  I can&#8217;t say this for sure of course because I have surveyed every single language that supports one or both of these databases, but you get the idea.</p>\n<p>For the rest of the items they do a reasonable job, although they do leave out some additional features that might be of interest to some like domains, inheritance, sequences, etc.</p>\n<p><strong>Summary</strong><br />\nThere are some features that are extremely handy even for fairly small databases (like views).  Look, MySQL does a pretty good job (aside from some issues) at what it is intended to do, stop trying to make excuses for though when you discover that it doesn&#8217;t fair well feature wise.</p>\n<p><strong>Benchmarks</strong><br />\nThere is no indication into how much tuning was done for either MySQL or PostgreSQL, if any was done at all.</p>\n<p><strong>Data Set &amp; Results</strong>: There is no discussion about how many simultaneous connections/users there are.  From the look of things all of these tests were done with a single connection/user.  If that is the case then all of the inserts done in this should be thrown out and redone using MySQL&#8217;s <a href=\"http://dev.mysql.com/doc/mysql/en/load-data.html\">LOAD DATA INFILE</a> command and PostgreSQL&#8217;s <a href=\"http://www.postgresql.org/docs/8.0/interactive/sql-copy.html\">COPY</a> command.  Doing bulk imports like this is always going to perform poorly using straight inserts.  Not to mention that if you are only interested with single connection/user situations then you may well include things like MS Access for your benchmarks.</p>\n<p>Another missing component is what type of table is being used in MySQL.  Unless you specify what <a href=\"http://dev.mysql.com/doc/mysql/en/storage-engines.html\">table type</a> you want MySQL will use the default type, which is MyISAM.  If the MySQL side of these tests were run using MyISAM tables then this whole test needs to be thrown out (inserts, queries and deletes) and redone using the InnoDB table type.</p>\n<p>Until these sorts of issues are at least addressed all of the results should simply be ignored, there is simply not enough information to gain anything remotely useful from those numbers at this point.</p>\n<p><strong>Conclusion</strong><br />\nAgain, if you are interested in making bulk imports happen very quickly, at least use the right tool for the job.</p>\n<p><strong>Hardware &amp; Software</strong><br />\nKudos for mentioning hardware and software details.</p>\n<p><strong>My Conclusion</strong><br />\nGoing good and meaningful benchmarks is hard work, taking a vastly over simplified approach like the one done here is not really helpful to anyone.  There were no goals outlined as to what sort of usage they wanted to test against (although it looks like there were interested in single user heavy insert models) and insufficient discussion and details about how they were going to mimic that model as accurately as possible.  Of the discussion that was provided, most it revolved around how to minimize the impact of rapid inserts wrapped in transactions for PostgreSQL.  That discussion is waste because they weren&#8217;t using the right tool for the right job.</p>\n<p>I was pointed to the review by <a href=\"http://www.sitepoint.com/blog-post-view?id=259681\">a blog entry at SitePoint</a>, which now has several comments.  There is something of thread revolving around an issue with MySQL where if you try insert a 300 character long string into a field that only supports 250 characters, MySQL will simply truncate your data without throwing an error.  This has been brought up before and it is simply wrong, the MySQL folks need to just fix this and move on instead of trying to find different ways to justify trashing your data when it is inserted.  A counter point is brought up that good programmers always validate their data before attempting to do an insert, in this case making sure that your string is less than or equal to 250 characters.  The sad thing about this stance is that there is some truth to it, but not in the sense that it is being used.  It&#8217;s true that you should be checking for obvious problems in your data before you insert it so that you can give meaningful errors back to the user, however, that doesn&#8217;t change the fact that what MySQL is doing is corrupting data on insert.</p>\n<p>This disagreement reminds me of the folks who simply add client side javascript error checking for form input, which allowed them to provide meaningful warnings and errors without having to process the form every time.  The security folks were quick to point out that the same checks still had to be done on the server side because client side javascript checks were easy to by pass.  Client side checks are a great thing for user, but they are no excuse to avoid those same checks on the server side.  Checking the length of your strings is a good idea to provide good user feedback, but it is not excuse to allow your database to corrupt data.</p>\n<p>Use the Microsoft test here, if MS SQL Server did this sort of thing, would you still be saying the same thing?</p>\n",
            "content_text": "Once again there is another attempt at comparing MySQL vs. PostgreSQL.  Like so many others before it, this benchmark falls prey to a classic mistake, not enough details.  So lets go through their review shall we?\nComparison\nOperating System: All of the OSs listed for MySQL also apply to PostgreSQL.  No explanation is given as to why they only listed two for PostgreSQL.  There are even FAQ entries for some OSs, like AIX, HPUX, IRIX and Solaris.\nPerformance: I&#8217;ll go into more detail on this when later, suffice it say that this is a gross over simplification.\nOther APIs: I&#8217;m not sure why they describe MySQL with &#8220;Most of languages&#8221; and mention specific languages for PostgreSQL.  I suspect that virtually any language that has MySQL support also has PostgreSQL support.  I can&#8217;t say this for sure of course because I have surveyed every single language that supports one or both of these databases, but you get the idea.\nFor the rest of the items they do a reasonable job, although they do leave out some additional features that might be of interest to some like domains, inheritance, sequences, etc.\nSummary\nThere are some features that are extremely handy even for fairly small databases (like views).  Look, MySQL does a pretty good job (aside from some issues) at what it is intended to do, stop trying to make excuses for though when you discover that it doesn&#8217;t fair well feature wise.\nBenchmarks\nThere is no indication into how much tuning was done for either MySQL or PostgreSQL, if any was done at all.\nData Set &amp; Results: There is no discussion about how many simultaneous connections/users there are.  From the look of things all of these tests were done with a single connection/user.  If that is the case then all of the inserts done in this should be thrown out and redone using MySQL&#8217;s LOAD DATA INFILE command and PostgreSQL&#8217;s COPY command.  Doing bulk imports like this is always going to perform poorly using straight inserts.  Not to mention that if you are only interested with single connection/user situations then you may well include things like MS Access for your benchmarks.\nAnother missing component is what type of table is being used in MySQL.  Unless you specify what table type you want MySQL will use the default type, which is MyISAM.  If the MySQL side of these tests were run using MyISAM tables then this whole test needs to be thrown out (inserts, queries and deletes) and redone using the InnoDB table type.\nUntil these sorts of issues are at least addressed all of the results should simply be ignored, there is simply not enough information to gain anything remotely useful from those numbers at this point.\nConclusion\nAgain, if you are interested in making bulk imports happen very quickly, at least use the right tool for the job.\nHardware &amp; Software\nKudos for mentioning hardware and software details.\nMy Conclusion\nGoing good and meaningful benchmarks is hard work, taking a vastly over simplified approach like the one done here is not really helpful to anyone.  There were no goals outlined as to what sort of usage they wanted to test against (although it looks like there were interested in single user heavy insert models) and insufficient discussion and details about how they were going to mimic that model as accurately as possible.  Of the discussion that was provided, most it revolved around how to minimize the impact of rapid inserts wrapped in transactions for PostgreSQL.  That discussion is waste because they weren&#8217;t using the right tool for the right job.\nI was pointed to the review by a blog entry at SitePoint, which now has several comments.  There is something of thread revolving around an issue with MySQL where if you try insert a 300 character long string into a field that only supports 250 characters, MySQL will simply truncate your data without throwing an error.  This has been brought up before and it is simply wrong, the MySQL folks need to just fix this and move on instead of trying to find different ways to justify trashing your data when it is inserted.  A counter point is brought up that good programmers always validate their data before attempting to do an insert, in this case making sure that your string is less than or equal to 250 characters.  The sad thing about this stance is that there is some truth to it, but not in the sense that it is being used.  It&#8217;s true that you should be checking for obvious problems in your data before you insert it so that you can give meaningful errors back to the user, however, that doesn&#8217;t change the fact that what MySQL is doing is corrupting data on insert.\nThis disagreement reminds me of the folks who simply add client side javascript error checking for form input, which allowed them to provide meaningful warnings and errors without having to process the form every time.  The security folks were quick to point out that the same checks still had to be done on the server side because client side javascript checks were easy to by pass.  Client side checks are a great thing for user, but they are no excuse to avoid those same checks on the server side.  Checking the length of your strings is a good idea to provide good user feedback, but it is not excuse to allow your database to corrupt data.\nUse the Microsoft test here, if MS SQL Server did this sort of thing, would you still be saying the same thing?",
            "date_published": "2005-05-03T09:28:38-06:00",
            "date_modified": "2005-05-03T09:28:38-06:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "MySQL",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/04/18/oracle-and-apple/",
            "url": "https://blog.josephscott.org/2005/04/18/oracle-and-apple/",
            "title": "Oracle And Apple",
            "content_html": "<p>It seems that there are still plenty of applications where only the big boys (DB2 and Oracle) really succeed.  As much as I like to apply PostgreSQL (or even MySQL) to back end solutions, there are still times when one of those insanely advanced (and usually complex) features are needed to meet your requirements.  Oracle being the biggest player on the block is often the one turned to to tackle those types of problems.  These installs often end up on &#8220;big&#8221; hardware, which puts the idea of <a href=\"http://psoug.org/rac_on_mac.html\">Oracle on Apple</a> in an interesting position.  The new Apple server hardware seems to perform well with a more reasonable cost than the traditional big hardware players (Sun, IBM, large x86, etc).</p>\n<p>Remember what a big splash the <a href=\"http://www.tcf.vt.edu/systemX.html\">Virginia Tech Apple supercomputer</a> made when it was first announced?  It wouldn&#8217;t surprise me if someone is doing something similar for their database back end using Oracle on top of a whole bunch of Apple hardware.</p>\n<p>The Virginia Tech system is still ranked #7 on the <a href=\"http://www.top500.org/lists/plists.php?Y=2004&amp;M=11\">Top 500</a> list.</p>\n",
            "content_text": "It seems that there are still plenty of applications where only the big boys (DB2 and Oracle) really succeed.  As much as I like to apply PostgreSQL (or even MySQL) to back end solutions, there are still times when one of those insanely advanced (and usually complex) features are needed to meet your requirements.  Oracle being the biggest player on the block is often the one turned to to tackle those types of problems.  These installs often end up on &#8220;big&#8221; hardware, which puts the idea of Oracle on Apple in an interesting position.  The new Apple server hardware seems to perform well with a more reasonable cost than the traditional big hardware players (Sun, IBM, large x86, etc).\nRemember what a big splash the Virginia Tech Apple supercomputer made when it was first announced?  It wouldn&#8217;t surprise me if someone is doing something similar for their database back end using Oracle on top of a whole bunch of Apple hardware.\nThe Virginia Tech system is still ranked #7 on the Top 500 list.",
            "date_published": "2005-04-18T06:51:50-06:00",
            "date_modified": "2005-04-18T06:51:50-06:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "Mac OS X",
                "OS"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/02/11/mysql-performance-on-linux-opennetfreebsd-and-solaris/",
            "url": "https://blog.josephscott.org/2005/02/11/mysql-performance-on-linux-opennetfreebsd-and-solaris/",
            "title": "MySQL Performance On Linux, (Open|Net|Free)BSD and Solaris",
            "content_html": "<p>The world of benchmarks is fraught with peril, those who enter are likely to get flamed, not matter what the results are.  I had already prepared myself for the worst when I saw the title of <a href=\"http://software.newsforge.com/software/04/12/27/1238216.shtml?tid=152&amp;tid=72&amp;tid=29\">Using MySQL to benchmark OS performance</a> on NewsForge the other day.  After reading through it, and part 2, <a href=\"http://software.newsforge.com/article.pl?sid=04/12/27/1243207\">Comparing MySQL performance</a>, I was pleasantly surprised.  At the end of the article I got the feeling that <a href=\"http://vegan.net/tony/\">Tony Bourke</a> had made an honest attempt at testing MySQL 4.0.22 on the following operating systems:</p>\n<ul>\n<li>FreeBSD 4.11</li>\n<li>FreeBSD 5.3</li>\n<li>NetBSD 2.0</li>\n<li>Linux 2.6</li>\n<li>Linux 2.4</li>\n<li>Solaris 10 x86 (build 69)</li>\n<li>OpenBSD 3.6</li>\n</ul>\n<p>While there are things that I&#8217;d recommend doing differently, it certainly seems like Tony did a good job to trying to make this as balanced as possible.  Perhaps my biggest beef with his methods was the decision to run all of the tests locally, instead over the network.  To his credit he does a good job explaining why he ended up not doing so, but that doesn&#8217;t change the fact that for those building apps (web or otherwise), don&#8217;t usually run that application on the same system that is running MySQL.</p>\n<p>The results of the test still feel a little bit odd.  I can&#8217;t really hold this against Tony though, I&#8217;m sure he was working on a deadline and if you put off publishing forever then why bother doing it in the first place.  That said, I suspect that there is more that could be done if more time and resources were available.  Some of the other obvious possibilities include using MySQL built for that OS (rpm&#8217;s, BSD ports, etc), looking at additional file system tweaks and differences (does Linux still default to async fs mounts?) and trying different versions of MySQL (4.1 just went into production, but 5.x betas have been around for awhile too).</p>\n",
            "content_text": "The world of benchmarks is fraught with peril, those who enter are likely to get flamed, not matter what the results are.  I had already prepared myself for the worst when I saw the title of Using MySQL to benchmark OS performance on NewsForge the other day.  After reading through it, and part 2, Comparing MySQL performance, I was pleasantly surprised.  At the end of the article I got the feeling that Tony Bourke had made an honest attempt at testing MySQL 4.0.22 on the following operating systems:\n\nFreeBSD 4.11\nFreeBSD 5.3\nNetBSD 2.0\nLinux 2.6\nLinux 2.4\nSolaris 10 x86 (build 69)\nOpenBSD 3.6\n\nWhile there are things that I&#8217;d recommend doing differently, it certainly seems like Tony did a good job to trying to make this as balanced as possible.  Perhaps my biggest beef with his methods was the decision to run all of the tests locally, instead over the network.  To his credit he does a good job explaining why he ended up not doing so, but that doesn&#8217;t change the fact that for those building apps (web or otherwise), don&#8217;t usually run that application on the same system that is running MySQL.\nThe results of the test still feel a little bit odd.  I can&#8217;t really hold this against Tony though, I&#8217;m sure he was working on a deadline and if you put off publishing forever then why bother doing it in the first place.  That said, I suspect that there is more that could be done if more time and resources were available.  Some of the other obvious possibilities include using MySQL built for that OS (rpm&#8217;s, BSD ports, etc), looking at additional file system tweaks and differences (does Linux still default to async fs mounts?) and trying different versions of MySQL (4.1 just went into production, but 5.x betas have been around for awhile too).",
            "date_published": "2005-02-11T14:37:26-07:00",
            "date_modified": "2005-02-11T14:37:26-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "FreeBSD",
                "Linux",
                "MySQL",
                "OS"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/01/19/postgresql-80/",
            "url": "https://blog.josephscott.org/2005/01/19/postgresql-80/",
            "title": "PostgreSQL 8.0",
            "content_html": "<p>I&#8217;ve very happy to see that PostgreSQL 8.0 is now <a href=\"http://www.postgresql.org/about/news.277\">officially released</a>.  There are all sorts of <a href=\"http://www.postgresql.org/docs/8.0/static/release.html#RELEASE-8-0\">new features in 8.0</a> in several different areas.  Here are some of the highlights:</p>\n<ul>\n<li>Microsoft Windows Native Server</li>\n<li>Savepoints</li>\n<li>Point-In-Time Recovery</li>\n<li>Tablespaces</li>\n<li>Improved Buffer Management, CHECKPOINT, VACUUM</li>\n<li>Change Column Types</li>\n<li>New Perl Server-Side Language</li>\n<li>Comma-separated-value (CSV) support in COPY</li>\n<li>Support cross-data-type index usage</li>\n<li>Add ability to prolong vacuum to reduce performance impact</li>\n<li>Implement dollar quoting to simplify single-quote</li>\n</ul>\n<p>Congrats to the whole PostgreSQL team, thank you for all of your hard work and diligence.</p>\n<p><strong>UPDATE 11:30am 19 Jan 2005</strong>: The announcement of PostgreSQL 8.0 is very popular, my <a href=\"http://www.pubsub.com/\">PubSub</a> watch list for &#8220;PostgreSQL&#8221; is just exploding with entries today.</p>\n",
            "content_text": "I&#8217;ve very happy to see that PostgreSQL 8.0 is now officially released.  There are all sorts of new features in 8.0 in several different areas.  Here are some of the highlights:\n\nMicrosoft Windows Native Server\nSavepoints\nPoint-In-Time Recovery\nTablespaces\nImproved Buffer Management, CHECKPOINT, VACUUM\nChange Column Types\nNew Perl Server-Side Language\nComma-separated-value (CSV) support in COPY\nSupport cross-data-type index usage\nAdd ability to prolong vacuum to reduce performance impact\nImplement dollar quoting to simplify single-quote\n\nCongrats to the whole PostgreSQL team, thank you for all of your hard work and diligence.\nUPDATE 11:30am 19 Jan 2005: The announcement of PostgreSQL 8.0 is very popular, my PubSub watch list for &#8220;PostgreSQL&#8221; is just exploding with entries today.",
            "date_published": "2005-01-19T10:29:55-07:00",
            "date_modified": "2005-01-19T10:29:55-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/01/16/whats-new-in-mysql-41/",
            "url": "https://blog.josephscott.org/2005/01/16/whats-new-in-mysql-41/",
            "title": "What\u2019s New In MySQL 4.1",
            "content_html": "<p>Back in October 2004 <a href=\"http://www.mysql.com/news-and-events/press-release/release_2004_32.html\">MySQL 4.1</a> was announced as being the officially ready for production.  I&#8217;ve got a new MySQL install to do so I wanted to take the time to go through the new features in more detail.  There are all sorts of good things in 4.1:</p>\n<ul>\n<li><a href=\"http://dev.mysql.com/tech-resources/articles/4.1/gis-with-mysql.html\">GIS</a>: Although I&#8217;ve never done any development using GIS, it has always fascinated me.  One of those things on my list of things to do <img src=\"https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png\" alt=\"\ud83d\ude42\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /> </li>\n<li><a href=\"http://dev.mysql.com/tech-resources/articles/4.1/unicode.html\">Unicode</a>: If you&#8217;ve got to deal with funky characters, you&#8217;ve got to have this.</li>\n<li><a href=\"http://dev.mysql.com/tech-resources/articles/4.1/installer.html\">New Windows Install</a>: I&#8217;ve run MySQL on Windows before, but only for development.</li>\n<li><a href=\"http://dev.mysql.com/tech-resources/articles/4.1/prepared-statements.html\">Prepared Statements</a>: Looks like just this is just a first step, later versions are supposed to support query plan caching for better performance.</li>\n<li><a href=\"http://dev.mysql.com/tech-resources/articles/4.1/time.html\">Temporal Functionality</a>: More time zone options and finer grained time resolution.</li>\n<li><a href=\"http://dev.mysql.com/tech-resources/articles/4.1/subqueries.html\">Subqueries (Sub-Selects)</a>: I can&#8217;t tell you how much I&#8217;ve wanted this in MySQL.  This one feature is almost enough for me to make MySQL 4.1 the minimum version for some projects.</li>\n</ul>\n<p>There is also on article on <a href=\"http://dev.mysql.com/tech-resources/articles/4.1/grab-bag.html\">other little features</a>.  Some of these are pretty cool:</p>\n<ul>\n<li>Better help</li>\n<li>&#8216;ON DUPLICATE KEY UPDATE&#8217; is an interesting short cut, it makes an INSERT more intelligence without having to do additional queries.</li>\n<li>&#8216;GROUP BY &#8230; WITH ROLLUP&#8217; is great!  This provides you with the ability to get row sums right out of a SQL query.  I wonder what it take to get this ability added to PostgreSQL?</li>\n<li>A few handy new functions: COMPRESS(), UNCOMPRESS(), GROUP_CONCAT(), VARIANCE(), CRC32() and UUID().\n</li>\n</ul>\n<p>These aren&#8217;t all of the new features, but these stood out to me.  MySQL is obviously focusing on more of the traditional (read: Oracle) SQL database functionality that they&#8217;ve avoided for so long.  It&#8217;s good to see, it will certainly make some of my projects that make use of MySQL much easier.</p>\n",
            "content_text": "Back in October 2004 MySQL 4.1 was announced as being the officially ready for production.  I&#8217;ve got a new MySQL install to do so I wanted to take the time to go through the new features in more detail.  There are all sorts of good things in 4.1:\n\nGIS: Although I&#8217;ve never done any development using GIS, it has always fascinated me.  One of those things on my list of things to do  \nUnicode: If you&#8217;ve got to deal with funky characters, you&#8217;ve got to have this.\nNew Windows Install: I&#8217;ve run MySQL on Windows before, but only for development.\nPrepared Statements: Looks like just this is just a first step, later versions are supposed to support query plan caching for better performance.\nTemporal Functionality: More time zone options and finer grained time resolution.\nSubqueries (Sub-Selects): I can&#8217;t tell you how much I&#8217;ve wanted this in MySQL.  This one feature is almost enough for me to make MySQL 4.1 the minimum version for some projects.\n\nThere is also on article on other little features.  Some of these are pretty cool:\n\nBetter help\n&#8216;ON DUPLICATE KEY UPDATE&#8217; is an interesting short cut, it makes an INSERT more intelligence without having to do additional queries.\n&#8216;GROUP BY &#8230; WITH ROLLUP&#8217; is great!  This provides you with the ability to get row sums right out of a SQL query.  I wonder what it take to get this ability added to PostgreSQL?\nA few handy new functions: COMPRESS(), UNCOMPRESS(), GROUP_CONCAT(), VARIANCE(), CRC32() and UUID().\n\n\nThese aren&#8217;t all of the new features, but these stood out to me.  MySQL is obviously focusing on more of the traditional (read: Oracle) SQL database functionality that they&#8217;ve avoided for so long.  It&#8217;s good to see, it will certainly make some of my projects that make use of MySQL much easier.",
            "date_published": "2005-01-16T23:47:14-07:00",
            "date_modified": "2005-01-16T23:47:14-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "MySQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/01/03/benchmarks-and-dates/",
            "url": "https://blog.josephscott.org/2005/01/03/benchmarks-and-dates/",
            "title": "Benchmarks And Dates",
            "content_html": "<p>Benchmarks, love them or hate them, they are here to stay.  Though rarely (never?) perfect, they can be helpful and more often than not, misleading.  For now though, I want to comment on just one aspect of benchmarks: dates.  As a general rule, most people expect benchmarks to change over time because technology generally advances and gets better as time goes on.  For my rant on this subject I&#8217;m going to pick on the <a href=\"http://www.sqlite.org/speed.html\">SQLite Database Speed Comparison</a> because it is such a great example of why dates are important to a benchmark (and I am still seeing it <a href=\"http://www.gadgetopia.com/2004/12/31/LinuxDatabaseBenchmarks.html\">referenced</a>, as recently as 31 Dec 2004).</p>\n<p>For starters, there is no indication as to when this benchmark was performed.  Was it December 2004?  October 2003?  May 2001?  It doesn&#8217;t say.  Now presumably this benchmark would (should?) carry more weight if it was fairly recent and less weight the older it gets.  The only date I could find on the page was the last modified line at the bottom, 2004/10/10.  This might lead one to believe that this benchmark was done in the very recent past.  There are clues that lead me to believe this is not the case though, specifically the version numbers for the various database systems tested.  While I haven&#8217;t tracked the SQLite releases very well, I&#8217;m reasonably familiar with when PostgreSQL and MySQL releases have happened.  So I looked around to find out definitively when each version of the tested software was released.  Fortunately each one has a changelog available, with dates:</p>\n<p><strong>SQLite 2.7.6</strong>: 25 Jan 2003<br />\n<strong>PostgreSQL 7.1.3</strong>: 15 Aug 2001<br />\n<strong>MySQL 3.23.41</strong>: 11 Aug 2001</p>\n<p>The most obvious issue here is the huge range in dates.  While tested versions of PostgreSQL and MySQL were released in the same month, the SQLite version is 17 months newer than either of the other software versions.  At this point I would hazard a guess that this benchmark was probably done not long after January 2003, given the dates above.  On this merit alone I pretty much ignore the numbers generated by this benchmark because of the huge gap in development time.  We&#8217;re talking nearly a year and a half advantage given to SQLite over PostgreSQL and MySQL.  Who would possibly think that this would provide any sort of meaningful comparison for someone looking into database benchmarks?</p>\n<p>Just by the way of additional information, by January of 2003, MySQL had released version 4.0.10.  The MySQL 4.0.x branch was given the &#8216;Production&#8217; tag two months later in March.  Comparing it to 3.23.55 might have also been reasonable, it also came out in January 2003.  PostgreSQL released 7.3.5 on 4 February 2003, so they could have used 7.3.1 from December of 2002.</p>\n<p>This benchmark is now some where around two years old, making it almost useless for anyone looking for meaningful information today.  Of course with out a published date there is now way of knowing this for sure.  Unless you did a little bit of homework and looked at when the release of each software version was you could be lead to believe that this test was done three weeks ago.  Another important date feature is indicating when the tested version of each software was released.  Normally I might not include that as a requirement because it should be reasonable to assume that the most recent production level code available was the one being tested.  In this case however that is clearly not the case, so that fact needs to be disclosed.</p>\n<p>I&#8217;m not picking on SQLite as a product, just using their published benchmark as an example of how excluding date information is a fatal flaw in their document.  Notice I did not discuss any of the results or numbers, because in this case once I found out the pertinent dates I&#8217;m able to throw out pretty much the entire benchmark because of its age.</p>\n",
            "content_text": "Benchmarks, love them or hate them, they are here to stay.  Though rarely (never?) perfect, they can be helpful and more often than not, misleading.  For now though, I want to comment on just one aspect of benchmarks: dates.  As a general rule, most people expect benchmarks to change over time because technology generally advances and gets better as time goes on.  For my rant on this subject I&#8217;m going to pick on the SQLite Database Speed Comparison because it is such a great example of why dates are important to a benchmark (and I am still seeing it referenced, as recently as 31 Dec 2004).\nFor starters, there is no indication as to when this benchmark was performed.  Was it December 2004?  October 2003?  May 2001?  It doesn&#8217;t say.  Now presumably this benchmark would (should?) carry more weight if it was fairly recent and less weight the older it gets.  The only date I could find on the page was the last modified line at the bottom, 2004/10/10.  This might lead one to believe that this benchmark was done in the very recent past.  There are clues that lead me to believe this is not the case though, specifically the version numbers for the various database systems tested.  While I haven&#8217;t tracked the SQLite releases very well, I&#8217;m reasonably familiar with when PostgreSQL and MySQL releases have happened.  So I looked around to find out definitively when each version of the tested software was released.  Fortunately each one has a changelog available, with dates:\nSQLite 2.7.6: 25 Jan 2003\nPostgreSQL 7.1.3: 15 Aug 2001\nMySQL 3.23.41: 11 Aug 2001\nThe most obvious issue here is the huge range in dates.  While tested versions of PostgreSQL and MySQL were released in the same month, the SQLite version is 17 months newer than either of the other software versions.  At this point I would hazard a guess that this benchmark was probably done not long after January 2003, given the dates above.  On this merit alone I pretty much ignore the numbers generated by this benchmark because of the huge gap in development time.  We&#8217;re talking nearly a year and a half advantage given to SQLite over PostgreSQL and MySQL.  Who would possibly think that this would provide any sort of meaningful comparison for someone looking into database benchmarks?\nJust by the way of additional information, by January of 2003, MySQL had released version 4.0.10.  The MySQL 4.0.x branch was given the &#8216;Production&#8217; tag two months later in March.  Comparing it to 3.23.55 might have also been reasonable, it also came out in January 2003.  PostgreSQL released 7.3.5 on 4 February 2003, so they could have used 7.3.1 from December of 2002.\nThis benchmark is now some where around two years old, making it almost useless for anyone looking for meaningful information today.  Of course with out a published date there is now way of knowing this for sure.  Unless you did a little bit of homework and looked at when the release of each software version was you could be lead to believe that this test was done three weeks ago.  Another important date feature is indicating when the tested version of each software was released.  Normally I might not include that as a requirement because it should be reasonable to assume that the most recent production level code available was the one being tested.  In this case however that is clearly not the case, so that fact needs to be disclosed.\nI&#8217;m not picking on SQLite as a product, just using their published benchmark as an example of how excluding date information is a fatal flaw in their document.  Notice I did not discuss any of the results or numbers, because in this case once I found out the pertinent dates I&#8217;m able to throw out pretty much the entire benchmark because of its age.",
            "date_published": "2005-01-03T11:13:12-07:00",
            "date_modified": "2005-01-03T11:13:12-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "General",
                "Programming"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/12/16/postgresql-performance-of-where-exists/",
            "url": "https://blog.josephscott.org/2004/12/16/postgresql-performance-of-where-exists/",
            "title": "PostgreSQL Performance of \u201cwhere exists\u201d",
            "content_html": "<p>I&#8217;m always interested in bits about the performance of specific types of SQL queries, so I was curious when I came across <a href=\"http://wrschneider.blogspot.com/\">Bill Schneider</a>&#8216;s post about <a href=\"http://wrschneider.blogspot.com/2004/12/postgresql-performance-of-where-exists.html\">PostgreSQL performance of &#8220;where exists&#8221;</a>.  He was comparing the performance of two different queries that provide the same results, so I took a look at some of my data and converted them to fit data that I already had:</p>\n<p><strong>Query 1</strong></p>\n<pre>\nselect course_abbr from courses_basetable\nwhere exists (\n  select 1 from editions\n  where editions.course_abbr = courses_basetable.course_abbr\n)\n</pre>\n<p><strong>Query 2</strong></p>\n<pre>\nselect distinct(editions.course_abbr) from editions\njoin courses on\n  editions.course_abbr = courses.course_abbr\n</pre>\n<p>Both of these queries return the same list of course abbreviations, but perform slightly different.  To avoid the suspense I&#8217;m going to give away the ending now: Query 1 runs faster than Query 2 (tested using PostgreSQL 7.4.6), but for Bill Query 2 ran faster (using PostgreSQL 7.3.x).  For my tests Query 1 took an average time of 29 ms and Query 2 took an average of 31 ms.  So what, 2 ms you say, I throw that much CPU time away several times a minute!  The absolute times don&#8217;t really matter, the percentage difference does.  In this case Query 1 is more than 5% faster than Query 2.</p>\n<p>To find out more about why these two queries perform so differently I ran them both using <a href=\"http://www.postgresql.org/docs/current/static/sql-explain.html\">EXPLAIN ANALYZE</a> to see what was going on.  It should be rather obvious why Query 2 takes longer after seeing the EXPLAIN ANALYZE results:</p>\n<p><strong>Query 1</strong></p>\n<pre>\nSeq Scan on courses_basetable  (cost=0.00..35.82 rows=19 width=8)\n(actual time=0.369..9.485 rows=36 loops=1)\n  Filter: (subplan)\n  SubPlan\n    -&gt;  Seq Scan on editions  (cost=0.00..1.81 rows=2 width=0)\n          (actual time=0.207..0.207 rows=1 loops=38)\n          Filter: ((course_abbr)::text = ($0)::text)\n</pre>\n<p><strong>Query 2</strong></p>\n<pre>\nUnique  (cost=6.06..6.38 rows=36 width=8)\n(actual time=4.697..5.252 rows=36 loops=1)\n  -&gt;  Sort  (cost=6.06..6.22 rows=65 width=8)\n        (actual time=4.682..4.849 rows=65 loops=1)\n        Sort Key: editions.course_abbr\n        -&gt;  Hash Join  (cost=1.47..4.10 rows=65 width=8)\n              (actual time=1.652..3.357 rows=65 loops=1)\n              Hash Cond: ((\"outer\".course_abbr)::text = (\"inner\".course_abbr)::text)\n              -&gt;  Seq Scan on editions  (cost=0.00..1.65 rows=65 width=8)\n                    (actual time=0.017..0.462 rows=65 loops=1)\n              -&gt;  Hash  (cost=1.38..1.38 rows=38 width=8)\n                    (actual time=1.034..1.034 rows=0 loops=1)\n                    -&gt;  Seq Scan on courses_basetable  (cost=0.00..1.38 rows=38 width=8)\n                          (actual time=0.106..0.510 rows=38 loops=1)\n</pre>\n<p>Query 2 has more work to than Query 1.  If anyone else has other data points for this query let me know.</p>\n<p>On a side note, <a href=\"http://www.blogger.com/\">Blogger</a> really needs to support <a href=\"http://www.movabletype.org/trackback/\">TrackBack</a>, I&#8217;m tired of giving a response in a post and then having to leave a comment to let the person know where to find it.</p>\n",
            "content_text": "I&#8217;m always interested in bits about the performance of specific types of SQL queries, so I was curious when I came across Bill Schneider&#8216;s post about PostgreSQL performance of &#8220;where exists&#8221;.  He was comparing the performance of two different queries that provide the same results, so I took a look at some of my data and converted them to fit data that I already had:\nQuery 1\n\nselect course_abbr from courses_basetable\nwhere exists (\n  select 1 from editions\n  where editions.course_abbr = courses_basetable.course_abbr\n)\n\nQuery 2\n\nselect distinct(editions.course_abbr) from editions\njoin courses on\n  editions.course_abbr = courses.course_abbr\n\nBoth of these queries return the same list of course abbreviations, but perform slightly different.  To avoid the suspense I&#8217;m going to give away the ending now: Query 1 runs faster than Query 2 (tested using PostgreSQL 7.4.6), but for Bill Query 2 ran faster (using PostgreSQL 7.3.x).  For my tests Query 1 took an average time of 29 ms and Query 2 took an average of 31 ms.  So what, 2 ms you say, I throw that much CPU time away several times a minute!  The absolute times don&#8217;t really matter, the percentage difference does.  In this case Query 1 is more than 5% faster than Query 2.\nTo find out more about why these two queries perform so differently I ran them both using EXPLAIN ANALYZE to see what was going on.  It should be rather obvious why Query 2 takes longer after seeing the EXPLAIN ANALYZE results:\nQuery 1\n\nSeq Scan on courses_basetable  (cost=0.00..35.82 rows=19 width=8)\n(actual time=0.369..9.485 rows=36 loops=1)\n  Filter: (subplan)\n  SubPlan\n    -&gt;  Seq Scan on editions  (cost=0.00..1.81 rows=2 width=0)\n          (actual time=0.207..0.207 rows=1 loops=38)\n          Filter: ((course_abbr)::text = ($0)::text)\n\nQuery 2\n\nUnique  (cost=6.06..6.38 rows=36 width=8)\n(actual time=4.697..5.252 rows=36 loops=1)\n  -&gt;  Sort  (cost=6.06..6.22 rows=65 width=8)\n        (actual time=4.682..4.849 rows=65 loops=1)\n        Sort Key: editions.course_abbr\n        -&gt;  Hash Join  (cost=1.47..4.10 rows=65 width=8)\n              (actual time=1.652..3.357 rows=65 loops=1)\n              Hash Cond: ((\"outer\".course_abbr)::text = (\"inner\".course_abbr)::text)\n              -&gt;  Seq Scan on editions  (cost=0.00..1.65 rows=65 width=8)\n                    (actual time=0.017..0.462 rows=65 loops=1)\n              -&gt;  Hash  (cost=1.38..1.38 rows=38 width=8)\n                    (actual time=1.034..1.034 rows=0 loops=1)\n                    -&gt;  Seq Scan on courses_basetable  (cost=0.00..1.38 rows=38 width=8)\n                          (actual time=0.106..0.510 rows=38 loops=1)\n\nQuery 2 has more work to than Query 1.  If anyone else has other data points for this query let me know.\nOn a side note, Blogger really needs to support TrackBack, I&#8217;m tired of giving a response in a post and then having to leave a comment to let the person know where to find it.",
            "date_published": "2004-12-16T17:29:48-07:00",
            "date_modified": "2004-12-16T17:29:48-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/12/10/postgresql-vacuum/",
            "url": "https://blog.josephscott.org/2004/12/10/postgresql-vacuum/",
            "title": "PostgreSQL Vacuum",
            "content_html": "<p>Thanks to MVCC PostgreSQL does a pretty good job at avoiding locking contentions.  Unfortunately this ability does not come without a cost.  In this case the cost is having to run vacuum from time to time (the best interval for you will depend on usage) to recover rows and make that space available again.  This is especially true on a table that is changing a lot (INSERT, UPDATE and DELETE).  If you don&#8217;t do this you&#8217;ll notice things slowing down, just like <a href=\"http://www.livejournal.com/users/karl/7058.html\">Karl noticed with SELECT count(*)</a>.  Setting aside the question of doing SELECT count(*) on a table for now, if you have a table that is getting millions of changes a day then you better be looking at how often you need to run vacuum.  If it takes you years to hit a million changes then running vacuum won&#8217;t need to be as frequent.</p>\n<p>I&#8217;ve not played with pg_autovacuum yet, but that is another resource to look at for determining how often you need to vacuum your tables.</p>\n",
            "content_text": "Thanks to MVCC PostgreSQL does a pretty good job at avoiding locking contentions.  Unfortunately this ability does not come without a cost.  In this case the cost is having to run vacuum from time to time (the best interval for you will depend on usage) to recover rows and make that space available again.  This is especially true on a table that is changing a lot (INSERT, UPDATE and DELETE).  If you don&#8217;t do this you&#8217;ll notice things slowing down, just like Karl noticed with SELECT count(*).  Setting aside the question of doing SELECT count(*) on a table for now, if you have a table that is getting millions of changes a day then you better be looking at how often you need to run vacuum.  If it takes you years to hit a million changes then running vacuum won&#8217;t need to be as frequent.\nI&#8217;ve not played with pg_autovacuum yet, but that is another resource to look at for determining how often you need to vacuum your tables.",
            "date_published": "2004-12-10T11:42:34-07:00",
            "date_modified": "2004-12-10T11:42:34-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/11/30/the-metadata-problem-with-databases/",
            "url": "https://blog.josephscott.org/2004/11/30/the-metadata-problem-with-databases/",
            "title": "The Metadata Problem With Databases",
            "content_html": "<p>It is the end of 2004 and virtually everyone uses databases to back end <a href=\"http://www.m-w.com/cgi-bin/dictionary?book=Dictionary&amp;va=gobs&amp;x=15&amp;y=15\">gobs</a> data.  Lately there has even been talk of &#8220;the database&#8221; being <a href=\"http://www.rittman.net/archives/001130.html\">legacy technology</a>, a small piece of the more exciting whole of an application.  Mix a little bit of this thinking, my recent attempts to store <a href=\"http://joseph.randomnetworks.com/archives/2004/11/26/exposing-postgresql-regular-expression-check-constraints/\">column regular expressions in PostgreSQL constraints</a> and a lot of day time work being spent on in-house apps interfacing with databases and you end up with this question.  Where should I put my metadata?</p>\n<p>Let me make sure that I&#8217;m clear on exactly what metadata I&#8217;m interested in (for now).  Fields in a table generally have five attributes that we are commonly interested in: type, length, precision, nullness (I don&#8217;t think that is a word, but you know what I mean) and comment.  That is all fine and good, but I find myself wanting more, that is what got me started on using <a href=\"http://joseph.randomnetworks.com/archives/2004/05/24/postgresql-check-constraint-supports-regular-expressions/\">regular expressions in constraints</a> in the first place.  I figured that might be a good place to store another field attribute and get the added bonus of having the database enforce it also.  To start putting pen to paper here is an example table that we will work with:</p>\n<blockquote>\n<pre>\nCREATE TABLE users (\n  user_id SERIAL NOT NULL,\n  username VARCHAR(16) NOT NULL,\n  lastname VARCHAR(50) NOT NULL,\n  firstname VARCHAR(50) NULL,\n  email_addr VARCHAR(250) NULL\n);\n</pre>\n</blockquote>\n<p>Now imagine that you are writing a front end to this particular table (it could be web based, but that doesn&#8217;t matter) what sort additional attributes (more metadata!) would be handy to have?  Regular expression limitations were one the first things on my list.  I mean it is great that a username can only be 16 characters long, but what if only want to allow lowercase a through z and digits?  A simple regex takes care of this: &#8216;^[a-z0-9]+$&#8217;.  We could conceivably want to impose additional limitations on lastname, firstname and email_addr as well.  Moving on we discover that lastname is a great database column name, but a crummy field label to use in an application, so every where that the lastname field is used we hard code a nicer label, like &#8216;Last Name&#8217;.  This leaves me with a really unpleasant feeling, I&#8217;d much rather be able to ask for correct label to use everywhere that I&#8217;m displaying data for lastname.  This problem could potentially be solved using the column comment feature in databases.  So far so good, we could potentially store these two types of metadata in the database itself.</p>\n<p>We have pretty much used up on places to put additional metadata, so what happens when we need to store more?  One example would be something like an extended help or hint (bubble help?) for a field.  In my applications are work I would like to be able to use to this to explain what a field is in greater detail (like why an insert or an update will fail if you try to add a username that doesn&#8217;t match &#8216;^[a-z0-9]+$&#8217;).  I could use the comment field for this, but then I&#8217;m already using that for something else remember?  For now that is the extent of the additional metadata I&#8217;m interested, but it would not be unreasonable to have additional metadata that I would be interested in later on.</p>\n<p>Let use assume for a moment that all of the above issues can be satisfied, there is another concern I have with solutions I&#8217;ve outlined so far: metadata duplication.  Going back to our example table, say we want to limit lastname to match &#8216;^[a-zA-Z]+$&#8217;.  Looking around we discover that also have a customers table with a lastname field that we would also want to constrain in the same way, so add the same constraint to the customers.lastname as we did to users.lastname.  Later on we become more enlightened and realize that it is reasonable to have spaces in lastname so we change the regular expression to &#8216;^[a-zA-Z ]+$&#8217;, in two different places.  I almost hate to put it this way, but now our metadata is becoming heavily non-normalized.  With PostgreSQL this particular problem could be addressed by the use of <a href=\"http://www.postgresql.org/docs/current/static/sql-createdomain.html\">domains</a> because it supports check constraints, but that still would leave us with a shared label and long description (PostgreSQL domains don&#8217;t have a comment attribute).</p>\n<p>After going down this road awhile I came to the conclusion that I wasn&#8217;t going to be able to store all of the metadata that I wanted to using built-in database features.  My next thought was to put this in the business logic layer of an application.  I even half convinced myself this might be a good idea, but then that left me with a database that wouldn&#8217;t be able to enforce the regular expressions I was interested in.  So I went back to idea of finding my original goal of finding a way to store the metadata in the database (with regex constraint support).  It turns out that the most obvious solution works with a little bit of tweaking and one gotcha.</p>\n<p>What might be this obvious solution be?  Why a metadata table of course!  Something like this perhaps:</p>\n<blockquote>\n<pre>\nCREATE TABLE metadata (\n  metadata_id SERIAL NOT NULL,\n  column_name VARCHAR(100) NOT NULL,\n  display_name VARCHAR(250) NULL,\n  long_note TEXT NULL,\n  regex VARCHAR(1024) NULL\n);\n</pre>\n</blockquote>\n<p>With this table you could share common metadata and add to as needed in the future.  Then I looked at how to make this work with check constraints, so I tried something like this:</p>\n<blockquote>\n<pre>\nALTER TABLE users ADD CONSTRAINT\nusername ~ (\n  SELECT regex\n  FROM metadata\n  WHERE column_name = 'username'\n);\n</pre>\n</blockquote>\n<p>That idea got shot down real fast.  Unfortunately you can not do sub-queries in constraints on PostgreSQL.  For a moment I thought that I was going to be faced with defeat before I even got out the door.  After looking over more examples of constraints it looked like PostgreSQL would allow the use of a user defined function, so I tried this:</p>\n<blockquote>\n<pre>\nCREATE FUNCTION metadata_regex(varchar)\nRETURNS varchar AS '\n  SELECT regex\n  FROM metadata\n  WHERE column_name = $1;\n' LANGUAGE SQL;\nALTER TABLE users ADD CONSTRAINT\nusername_ck CHECK (\n  username ~ metadata_regex('username')\n);\n</pre>\n</blockquote>\n<p>For what ever reason this works where the plain sub-query does not.  There are a few nice features to this approach, you can set a constraint on every column without creating the metadata for it.  If the metadata_regex() function returns null then it is an insert/update will always succeed (as if there was no constraint there at all except for the small overhead of doing the lookup).  This was you can fill in metadata later if you want to.  My boss was quick to ask what happens when you change an existing regex that would invalidate existing data?  The short answer is that the system will let you change the regex and not give an error even with data that would now be invalid.  It will enforce the new regex when you insert new data or update old data.  Depending on your needs this may be a feature or a big problem.  If you need to catch these regex changes and throw errors if data would be invalid with new regex there are two possible solutions.  One solution would be to write a <a href=\"http://www.postgresql.org/docs/7.4/static/plpgsql-trigger.html\">trigger</a> what would verify that existing data would not be invalidated by the new regex and throw an error if it did.</p>\n<p>Another solution to the regex change problem is to <a href=\"http://www.postgresql.org/docs/7.4/static/plpgsql.html\">create functions</a> that serve as an API to the metadata; so all updates, inserts and deletes would be done via these functions instead of directly manipulating the metadata table.  These functions would act in a similar way as the trigger described above, preventing changes that would invalidate existing data.  I&#8217;m not wholly convinced that one is better than the other, but I am starting to lean towards the API idea because it allows you hide the details of how the metadata is stored.  This technique also lead me to another feature of PostgreSQL, <a href=\"http://www.postgresql.org/docs/7.4/static/sql-createtype.html\">creating new types</a>.</p>\n<p>Combining the API idea with creating new types allows you run regular SQL queries using the functions like tables.  For some examples of how this can work take a look at &#8216;<a href=\"http://www.varlena.com/varlena/GeneralBits/26.html\">Defining and returning Rowtypes</a>&#8216; in the 19 May 2003 General Bits.  There is another example at General Bits under &#8216;<a href=\"http://www.varlena.com/varlena/GeneralBits/73.php\">SQL 2003 Standards Approved</a>&#8216; from 17 April 2004.  Between these two examples you should start to see the possibilities here, by creating an API to all your additional metadata not only can you limit changes you can allow the database to enforce them, either via triggers or constraints and expose them to the application layer through traditional SQL queries.</p>\n<p>My goal is to be able to add new metadata (attributes) to database columns, allowing not just the upper application layers to make use of it but the database also.  After some research and pondering it looks like PostgreSQL has the features necessary to make this possible in an easy and extensible way.</p>\n",
            "content_text": "It is the end of 2004 and virtually everyone uses databases to back end gobs data.  Lately there has even been talk of &#8220;the database&#8221; being legacy technology, a small piece of the more exciting whole of an application.  Mix a little bit of this thinking, my recent attempts to store column regular expressions in PostgreSQL constraints and a lot of day time work being spent on in-house apps interfacing with databases and you end up with this question.  Where should I put my metadata?\nLet me make sure that I&#8217;m clear on exactly what metadata I&#8217;m interested in (for now).  Fields in a table generally have five attributes that we are commonly interested in: type, length, precision, nullness (I don&#8217;t think that is a word, but you know what I mean) and comment.  That is all fine and good, but I find myself wanting more, that is what got me started on using regular expressions in constraints in the first place.  I figured that might be a good place to store another field attribute and get the added bonus of having the database enforce it also.  To start putting pen to paper here is an example table that we will work with:\n\n\nCREATE TABLE users (\n  user_id SERIAL NOT NULL,\n  username VARCHAR(16) NOT NULL,\n  lastname VARCHAR(50) NOT NULL,\n  firstname VARCHAR(50) NULL,\n  email_addr VARCHAR(250) NULL\n);\n\n\nNow imagine that you are writing a front end to this particular table (it could be web based, but that doesn&#8217;t matter) what sort additional attributes (more metadata!) would be handy to have?  Regular expression limitations were one the first things on my list.  I mean it is great that a username can only be 16 characters long, but what if only want to allow lowercase a through z and digits?  A simple regex takes care of this: &#8216;^[a-z0-9]+$&#8217;.  We could conceivably want to impose additional limitations on lastname, firstname and email_addr as well.  Moving on we discover that lastname is a great database column name, but a crummy field label to use in an application, so every where that the lastname field is used we hard code a nicer label, like &#8216;Last Name&#8217;.  This leaves me with a really unpleasant feeling, I&#8217;d much rather be able to ask for correct label to use everywhere that I&#8217;m displaying data for lastname.  This problem could potentially be solved using the column comment feature in databases.  So far so good, we could potentially store these two types of metadata in the database itself.\nWe have pretty much used up on places to put additional metadata, so what happens when we need to store more?  One example would be something like an extended help or hint (bubble help?) for a field.  In my applications are work I would like to be able to use to this to explain what a field is in greater detail (like why an insert or an update will fail if you try to add a username that doesn&#8217;t match &#8216;^[a-z0-9]+$&#8217;).  I could use the comment field for this, but then I&#8217;m already using that for something else remember?  For now that is the extent of the additional metadata I&#8217;m interested, but it would not be unreasonable to have additional metadata that I would be interested in later on.\nLet use assume for a moment that all of the above issues can be satisfied, there is another concern I have with solutions I&#8217;ve outlined so far: metadata duplication.  Going back to our example table, say we want to limit lastname to match &#8216;^[a-zA-Z]+$&#8217;.  Looking around we discover that also have a customers table with a lastname field that we would also want to constrain in the same way, so add the same constraint to the customers.lastname as we did to users.lastname.  Later on we become more enlightened and realize that it is reasonable to have spaces in lastname so we change the regular expression to &#8216;^[a-zA-Z ]+$&#8217;, in two different places.  I almost hate to put it this way, but now our metadata is becoming heavily non-normalized.  With PostgreSQL this particular problem could be addressed by the use of domains because it supports check constraints, but that still would leave us with a shared label and long description (PostgreSQL domains don&#8217;t have a comment attribute).\nAfter going down this road awhile I came to the conclusion that I wasn&#8217;t going to be able to store all of the metadata that I wanted to using built-in database features.  My next thought was to put this in the business logic layer of an application.  I even half convinced myself this might be a good idea, but then that left me with a database that wouldn&#8217;t be able to enforce the regular expressions I was interested in.  So I went back to idea of finding my original goal of finding a way to store the metadata in the database (with regex constraint support).  It turns out that the most obvious solution works with a little bit of tweaking and one gotcha.\nWhat might be this obvious solution be?  Why a metadata table of course!  Something like this perhaps:\n\n\nCREATE TABLE metadata (\n  metadata_id SERIAL NOT NULL,\n  column_name VARCHAR(100) NOT NULL,\n  display_name VARCHAR(250) NULL,\n  long_note TEXT NULL,\n  regex VARCHAR(1024) NULL\n);\n\n\nWith this table you could share common metadata and add to as needed in the future.  Then I looked at how to make this work with check constraints, so I tried something like this:\n\n\nALTER TABLE users ADD CONSTRAINT\nusername ~ (\n  SELECT regex\n  FROM metadata\n  WHERE column_name = 'username'\n);\n\n\nThat idea got shot down real fast.  Unfortunately you can not do sub-queries in constraints on PostgreSQL.  For a moment I thought that I was going to be faced with defeat before I even got out the door.  After looking over more examples of constraints it looked like PostgreSQL would allow the use of a user defined function, so I tried this:\n\n\nCREATE FUNCTION metadata_regex(varchar)\nRETURNS varchar AS '\n  SELECT regex\n  FROM metadata\n  WHERE column_name = $1;\n' LANGUAGE SQL;\nALTER TABLE users ADD CONSTRAINT\nusername_ck CHECK (\n  username ~ metadata_regex('username')\n);\n\n\nFor what ever reason this works where the plain sub-query does not.  There are a few nice features to this approach, you can set a constraint on every column without creating the metadata for it.  If the metadata_regex() function returns null then it is an insert/update will always succeed (as if there was no constraint there at all except for the small overhead of doing the lookup).  This was you can fill in metadata later if you want to.  My boss was quick to ask what happens when you change an existing regex that would invalidate existing data?  The short answer is that the system will let you change the regex and not give an error even with data that would now be invalid.  It will enforce the new regex when you insert new data or update old data.  Depending on your needs this may be a feature or a big problem.  If you need to catch these regex changes and throw errors if data would be invalid with new regex there are two possible solutions.  One solution would be to write a trigger what would verify that existing data would not be invalidated by the new regex and throw an error if it did.\nAnother solution to the regex change problem is to create functions that serve as an API to the metadata; so all updates, inserts and deletes would be done via these functions instead of directly manipulating the metadata table.  These functions would act in a similar way as the trigger described above, preventing changes that would invalidate existing data.  I&#8217;m not wholly convinced that one is better than the other, but I am starting to lean towards the API idea because it allows you hide the details of how the metadata is stored.  This technique also lead me to another feature of PostgreSQL, creating new types.\nCombining the API idea with creating new types allows you run regular SQL queries using the functions like tables.  For some examples of how this can work take a look at &#8216;Defining and returning Rowtypes&#8216; in the 19 May 2003 General Bits.  There is another example at General Bits under &#8216;SQL 2003 Standards Approved&#8216; from 17 April 2004.  Between these two examples you should start to see the possibilities here, by creating an API to all your additional metadata not only can you limit changes you can allow the database to enforce them, either via triggers or constraints and expose them to the application layer through traditional SQL queries.\nMy goal is to be able to add new metadata (attributes) to database columns, allowing not just the upper application layers to make use of it but the database also.  After some research and pondering it looks like PostgreSQL has the features necessary to make this possible in an easy and extensible way.",
            "date_published": "2004-11-30T12:32:57-07:00",
            "date_modified": "2004-11-30T12:32:57-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database"
            ]
        }
    ]
}