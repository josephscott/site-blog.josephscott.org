{
    "version": "https://jsonfeed.org/version/1.1",
    "user_comment": "This feed allows you to read the posts from this site in any feed reader that supports the JSON Feed format. To add this feed to your reader, copy the following URL -- https://blog.josephscott.org/category/database/postgresql/feed/json/ -- and add it your reader.",
    "home_page_url": "https://blog.josephscott.org/category/database/postgresql/",
    "feed_url": "https://blog.josephscott.org/category/database/postgresql/feed/json/",
    "language": "en-US",
    "title": "PostgreSQL &#8211; Joseph Scott",
    "items": [
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/05/04/oracle-10g-vs-postgresql-8-vs-mysql-5/",
            "url": "https://blog.josephscott.org/2005/05/04/oracle-10g-vs-postgresql-8-vs-mysql-5/",
            "title": "Oracle 10g vs PostgreSQL 8 vs MySQL 5",
            "content_html": "<p>In contrast to my <a href=\"http://joseph.randomnetworks.com/archives/2005/05/03/benchmarks-not-enough-details/\">recent rant about database comparisons</a>, I was pretty impressed with the <a href=\"http://www.suite101.com/article.cfm/oracle/115560\">Oracle 10g vs PostgreSQL 8 vs MySQL 5</a> article.  The two articles don&#8217;t compare the same issues, but I&#8217;d like to point out how well this one done.</p>\n<p>Right from the start the <a href=\"http://www.suite101.com/profile.cfm/lewisc\">author</a> states his bias upfront (he is a long time Oracle admin), along with what was being compared.  In this case the focus was on installing on a lower end Windows 2000 system.  All of the factors were spelled out and the scoring method was described in sufficient detail.  Each factor had some discussion and and a score for each database.</p>\n<p>Overall I&#8217;d consider this a good example of a database comparison, it has a well defined (and narrow) focus with an upfront scoring system.  My only complaint with this review was in the &#8220;Documentation and Getting Started Support&#8221; section.  Although PostgreSQL and MySQL basically have the same issues in the discussion area, the scores are different.  Given how well the rest of the review was done I don&#8217;t think this issue completely ruins the review, just provides a low point.</p>\n<p>I hope that the author, <a href=\"http://blogs.ittoolbox.com/oracle/guide/\">Lewis R Cunningham</a>, continues to do additional reviews of these three databases using this same format.</p>\n",
            "content_text": "In contrast to my recent rant about database comparisons, I was pretty impressed with the Oracle 10g vs PostgreSQL 8 vs MySQL 5 article.  The two articles don&#8217;t compare the same issues, but I&#8217;d like to point out how well this one done.\nRight from the start the author states his bias upfront (he is a long time Oracle admin), along with what was being compared.  In this case the focus was on installing on a lower end Windows 2000 system.  All of the factors were spelled out and the scoring method was described in sufficient detail.  Each factor had some discussion and and a score for each database.\nOverall I&#8217;d consider this a good example of a database comparison, it has a well defined (and narrow) focus with an upfront scoring system.  My only complaint with this review was in the &#8220;Documentation and Getting Started Support&#8221; section.  Although PostgreSQL and MySQL basically have the same issues in the discussion area, the scores are different.  Given how well the rest of the review was done I don&#8217;t think this issue completely ruins the review, just provides a low point.\nI hope that the author, Lewis R Cunningham, continues to do additional reviews of these three databases using this same format.",
            "date_published": "2005-05-04T23:31:37-06:00",
            "date_modified": "2005-05-04T23:31:37-06:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "MySQL",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/05/03/benchmarks-never-enough-details/",
            "url": "https://blog.josephscott.org/2005/05/03/benchmarks-not-enough-details/",
            "title": "Benchmarks, Not Enough Details",
            "content_html": "<p>Once again there is another attempt at comparing <a href=\"http://www.mysql.com/\">MySQL</a> vs. <a href=\"http://www.postgresql.org/\">PostgreSQL</a>.  Like so many others before it, this benchmark falls prey to a classic mistake, not enough details.  So lets go through their review shall we?</p>\n<p><strong>Comparison</strong><br />\n<strong>Operating System</strong>: All of the OSs listed for MySQL also apply to PostgreSQL.  No explanation is given as to why they only listed two for PostgreSQL.  There are even <a href=\"http://www.postgresql.org/docs/faq/\">FAQ</a> entries for some OSs, like <a href=\"http://www.postgresql.org/docs/faqs.FAQ_AIX.html\">AIX</a>, <a href=\"http://www.postgresql.org/docs/faqs.FAQ_HPUX.html\">HPUX</a>, <a href=\"http://www.postgresql.org/docs/faqs.FAQ_IRIX.html\">IRIX</a> and <a href=\"http://www.postgresql.org/docs/faqs.FAQ_Solaris.html\">Solaris</a>.</p>\n<p><strong>Performance</strong>: I&#8217;ll go into more detail on this when later, suffice it say that this is a gross over simplification.</p>\n<p><strong>Other APIs</strong>: I&#8217;m not sure why they describe MySQL with &#8220;Most of languages&#8221; and mention specific languages for PostgreSQL.  I suspect that virtually any language that has MySQL support also has PostgreSQL support.  I can&#8217;t say this for sure of course because I have surveyed every single language that supports one or both of these databases, but you get the idea.</p>\n<p>For the rest of the items they do a reasonable job, although they do leave out some additional features that might be of interest to some like domains, inheritance, sequences, etc.</p>\n<p><strong>Summary</strong><br />\nThere are some features that are extremely handy even for fairly small databases (like views).  Look, MySQL does a pretty good job (aside from some issues) at what it is intended to do, stop trying to make excuses for though when you discover that it doesn&#8217;t fair well feature wise.</p>\n<p><strong>Benchmarks</strong><br />\nThere is no indication into how much tuning was done for either MySQL or PostgreSQL, if any was done at all.</p>\n<p><strong>Data Set &amp; Results</strong>: There is no discussion about how many simultaneous connections/users there are.  From the look of things all of these tests were done with a single connection/user.  If that is the case then all of the inserts done in this should be thrown out and redone using MySQL&#8217;s <a href=\"http://dev.mysql.com/doc/mysql/en/load-data.html\">LOAD DATA INFILE</a> command and PostgreSQL&#8217;s <a href=\"http://www.postgresql.org/docs/8.0/interactive/sql-copy.html\">COPY</a> command.  Doing bulk imports like this is always going to perform poorly using straight inserts.  Not to mention that if you are only interested with single connection/user situations then you may well include things like MS Access for your benchmarks.</p>\n<p>Another missing component is what type of table is being used in MySQL.  Unless you specify what <a href=\"http://dev.mysql.com/doc/mysql/en/storage-engines.html\">table type</a> you want MySQL will use the default type, which is MyISAM.  If the MySQL side of these tests were run using MyISAM tables then this whole test needs to be thrown out (inserts, queries and deletes) and redone using the InnoDB table type.</p>\n<p>Until these sorts of issues are at least addressed all of the results should simply be ignored, there is simply not enough information to gain anything remotely useful from those numbers at this point.</p>\n<p><strong>Conclusion</strong><br />\nAgain, if you are interested in making bulk imports happen very quickly, at least use the right tool for the job.</p>\n<p><strong>Hardware &amp; Software</strong><br />\nKudos for mentioning hardware and software details.</p>\n<p><strong>My Conclusion</strong><br />\nGoing good and meaningful benchmarks is hard work, taking a vastly over simplified approach like the one done here is not really helpful to anyone.  There were no goals outlined as to what sort of usage they wanted to test against (although it looks like there were interested in single user heavy insert models) and insufficient discussion and details about how they were going to mimic that model as accurately as possible.  Of the discussion that was provided, most it revolved around how to minimize the impact of rapid inserts wrapped in transactions for PostgreSQL.  That discussion is waste because they weren&#8217;t using the right tool for the right job.</p>\n<p>I was pointed to the review by <a href=\"http://www.sitepoint.com/blog-post-view?id=259681\">a blog entry at SitePoint</a>, which now has several comments.  There is something of thread revolving around an issue with MySQL where if you try insert a 300 character long string into a field that only supports 250 characters, MySQL will simply truncate your data without throwing an error.  This has been brought up before and it is simply wrong, the MySQL folks need to just fix this and move on instead of trying to find different ways to justify trashing your data when it is inserted.  A counter point is brought up that good programmers always validate their data before attempting to do an insert, in this case making sure that your string is less than or equal to 250 characters.  The sad thing about this stance is that there is some truth to it, but not in the sense that it is being used.  It&#8217;s true that you should be checking for obvious problems in your data before you insert it so that you can give meaningful errors back to the user, however, that doesn&#8217;t change the fact that what MySQL is doing is corrupting data on insert.</p>\n<p>This disagreement reminds me of the folks who simply add client side javascript error checking for form input, which allowed them to provide meaningful warnings and errors without having to process the form every time.  The security folks were quick to point out that the same checks still had to be done on the server side because client side javascript checks were easy to by pass.  Client side checks are a great thing for user, but they are no excuse to avoid those same checks on the server side.  Checking the length of your strings is a good idea to provide good user feedback, but it is not excuse to allow your database to corrupt data.</p>\n<p>Use the Microsoft test here, if MS SQL Server did this sort of thing, would you still be saying the same thing?</p>\n",
            "content_text": "Once again there is another attempt at comparing MySQL vs. PostgreSQL.  Like so many others before it, this benchmark falls prey to a classic mistake, not enough details.  So lets go through their review shall we?\nComparison\nOperating System: All of the OSs listed for MySQL also apply to PostgreSQL.  No explanation is given as to why they only listed two for PostgreSQL.  There are even FAQ entries for some OSs, like AIX, HPUX, IRIX and Solaris.\nPerformance: I&#8217;ll go into more detail on this when later, suffice it say that this is a gross over simplification.\nOther APIs: I&#8217;m not sure why they describe MySQL with &#8220;Most of languages&#8221; and mention specific languages for PostgreSQL.  I suspect that virtually any language that has MySQL support also has PostgreSQL support.  I can&#8217;t say this for sure of course because I have surveyed every single language that supports one or both of these databases, but you get the idea.\nFor the rest of the items they do a reasonable job, although they do leave out some additional features that might be of interest to some like domains, inheritance, sequences, etc.\nSummary\nThere are some features that are extremely handy even for fairly small databases (like views).  Look, MySQL does a pretty good job (aside from some issues) at what it is intended to do, stop trying to make excuses for though when you discover that it doesn&#8217;t fair well feature wise.\nBenchmarks\nThere is no indication into how much tuning was done for either MySQL or PostgreSQL, if any was done at all.\nData Set &amp; Results: There is no discussion about how many simultaneous connections/users there are.  From the look of things all of these tests were done with a single connection/user.  If that is the case then all of the inserts done in this should be thrown out and redone using MySQL&#8217;s LOAD DATA INFILE command and PostgreSQL&#8217;s COPY command.  Doing bulk imports like this is always going to perform poorly using straight inserts.  Not to mention that if you are only interested with single connection/user situations then you may well include things like MS Access for your benchmarks.\nAnother missing component is what type of table is being used in MySQL.  Unless you specify what table type you want MySQL will use the default type, which is MyISAM.  If the MySQL side of these tests were run using MyISAM tables then this whole test needs to be thrown out (inserts, queries and deletes) and redone using the InnoDB table type.\nUntil these sorts of issues are at least addressed all of the results should simply be ignored, there is simply not enough information to gain anything remotely useful from those numbers at this point.\nConclusion\nAgain, if you are interested in making bulk imports happen very quickly, at least use the right tool for the job.\nHardware &amp; Software\nKudos for mentioning hardware and software details.\nMy Conclusion\nGoing good and meaningful benchmarks is hard work, taking a vastly over simplified approach like the one done here is not really helpful to anyone.  There were no goals outlined as to what sort of usage they wanted to test against (although it looks like there were interested in single user heavy insert models) and insufficient discussion and details about how they were going to mimic that model as accurately as possible.  Of the discussion that was provided, most it revolved around how to minimize the impact of rapid inserts wrapped in transactions for PostgreSQL.  That discussion is waste because they weren&#8217;t using the right tool for the right job.\nI was pointed to the review by a blog entry at SitePoint, which now has several comments.  There is something of thread revolving around an issue with MySQL where if you try insert a 300 character long string into a field that only supports 250 characters, MySQL will simply truncate your data without throwing an error.  This has been brought up before and it is simply wrong, the MySQL folks need to just fix this and move on instead of trying to find different ways to justify trashing your data when it is inserted.  A counter point is brought up that good programmers always validate their data before attempting to do an insert, in this case making sure that your string is less than or equal to 250 characters.  The sad thing about this stance is that there is some truth to it, but not in the sense that it is being used.  It&#8217;s true that you should be checking for obvious problems in your data before you insert it so that you can give meaningful errors back to the user, however, that doesn&#8217;t change the fact that what MySQL is doing is corrupting data on insert.\nThis disagreement reminds me of the folks who simply add client side javascript error checking for form input, which allowed them to provide meaningful warnings and errors without having to process the form every time.  The security folks were quick to point out that the same checks still had to be done on the server side because client side javascript checks were easy to by pass.  Client side checks are a great thing for user, but they are no excuse to avoid those same checks on the server side.  Checking the length of your strings is a good idea to provide good user feedback, but it is not excuse to allow your database to corrupt data.\nUse the Microsoft test here, if MS SQL Server did this sort of thing, would you still be saying the same thing?",
            "date_published": "2005-05-03T09:28:38-06:00",
            "date_modified": "2005-05-03T09:28:38-06:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "MySQL",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2005/01/19/postgresql-80/",
            "url": "https://blog.josephscott.org/2005/01/19/postgresql-80/",
            "title": "PostgreSQL 8.0",
            "content_html": "<p>I&#8217;ve very happy to see that PostgreSQL 8.0 is now <a href=\"http://www.postgresql.org/about/news.277\">officially released</a>.  There are all sorts of <a href=\"http://www.postgresql.org/docs/8.0/static/release.html#RELEASE-8-0\">new features in 8.0</a> in several different areas.  Here are some of the highlights:</p>\n<ul>\n<li>Microsoft Windows Native Server</li>\n<li>Savepoints</li>\n<li>Point-In-Time Recovery</li>\n<li>Tablespaces</li>\n<li>Improved Buffer Management, CHECKPOINT, VACUUM</li>\n<li>Change Column Types</li>\n<li>New Perl Server-Side Language</li>\n<li>Comma-separated-value (CSV) support in COPY</li>\n<li>Support cross-data-type index usage</li>\n<li>Add ability to prolong vacuum to reduce performance impact</li>\n<li>Implement dollar quoting to simplify single-quote</li>\n</ul>\n<p>Congrats to the whole PostgreSQL team, thank you for all of your hard work and diligence.</p>\n<p><strong>UPDATE 11:30am 19 Jan 2005</strong>: The announcement of PostgreSQL 8.0 is very popular, my <a href=\"http://www.pubsub.com/\">PubSub</a> watch list for &#8220;PostgreSQL&#8221; is just exploding with entries today.</p>\n",
            "content_text": "I&#8217;ve very happy to see that PostgreSQL 8.0 is now officially released.  There are all sorts of new features in 8.0 in several different areas.  Here are some of the highlights:\n\nMicrosoft Windows Native Server\nSavepoints\nPoint-In-Time Recovery\nTablespaces\nImproved Buffer Management, CHECKPOINT, VACUUM\nChange Column Types\nNew Perl Server-Side Language\nComma-separated-value (CSV) support in COPY\nSupport cross-data-type index usage\nAdd ability to prolong vacuum to reduce performance impact\nImplement dollar quoting to simplify single-quote\n\nCongrats to the whole PostgreSQL team, thank you for all of your hard work and diligence.\nUPDATE 11:30am 19 Jan 2005: The announcement of PostgreSQL 8.0 is very popular, my PubSub watch list for &#8220;PostgreSQL&#8221; is just exploding with entries today.",
            "date_published": "2005-01-19T10:29:55-07:00",
            "date_modified": "2005-01-19T10:29:55-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/12/16/postgresql-performance-of-where-exists/",
            "url": "https://blog.josephscott.org/2004/12/16/postgresql-performance-of-where-exists/",
            "title": "PostgreSQL Performance of \u201cwhere exists\u201d",
            "content_html": "<p>I&#8217;m always interested in bits about the performance of specific types of SQL queries, so I was curious when I came across <a href=\"http://wrschneider.blogspot.com/\">Bill Schneider</a>&#8216;s post about <a href=\"http://wrschneider.blogspot.com/2004/12/postgresql-performance-of-where-exists.html\">PostgreSQL performance of &#8220;where exists&#8221;</a>.  He was comparing the performance of two different queries that provide the same results, so I took a look at some of my data and converted them to fit data that I already had:</p>\n<p><strong>Query 1</strong></p>\n<pre>\nselect course_abbr from courses_basetable\nwhere exists (\n  select 1 from editions\n  where editions.course_abbr = courses_basetable.course_abbr\n)\n</pre>\n<p><strong>Query 2</strong></p>\n<pre>\nselect distinct(editions.course_abbr) from editions\njoin courses on\n  editions.course_abbr = courses.course_abbr\n</pre>\n<p>Both of these queries return the same list of course abbreviations, but perform slightly different.  To avoid the suspense I&#8217;m going to give away the ending now: Query 1 runs faster than Query 2 (tested using PostgreSQL 7.4.6), but for Bill Query 2 ran faster (using PostgreSQL 7.3.x).  For my tests Query 1 took an average time of 29 ms and Query 2 took an average of 31 ms.  So what, 2 ms you say, I throw that much CPU time away several times a minute!  The absolute times don&#8217;t really matter, the percentage difference does.  In this case Query 1 is more than 5% faster than Query 2.</p>\n<p>To find out more about why these two queries perform so differently I ran them both using <a href=\"http://www.postgresql.org/docs/current/static/sql-explain.html\">EXPLAIN ANALYZE</a> to see what was going on.  It should be rather obvious why Query 2 takes longer after seeing the EXPLAIN ANALYZE results:</p>\n<p><strong>Query 1</strong></p>\n<pre>\nSeq Scan on courses_basetable  (cost=0.00..35.82 rows=19 width=8)\n(actual time=0.369..9.485 rows=36 loops=1)\n  Filter: (subplan)\n  SubPlan\n    -&gt;  Seq Scan on editions  (cost=0.00..1.81 rows=2 width=0)\n          (actual time=0.207..0.207 rows=1 loops=38)\n          Filter: ((course_abbr)::text = ($0)::text)\n</pre>\n<p><strong>Query 2</strong></p>\n<pre>\nUnique  (cost=6.06..6.38 rows=36 width=8)\n(actual time=4.697..5.252 rows=36 loops=1)\n  -&gt;  Sort  (cost=6.06..6.22 rows=65 width=8)\n        (actual time=4.682..4.849 rows=65 loops=1)\n        Sort Key: editions.course_abbr\n        -&gt;  Hash Join  (cost=1.47..4.10 rows=65 width=8)\n              (actual time=1.652..3.357 rows=65 loops=1)\n              Hash Cond: ((\"outer\".course_abbr)::text = (\"inner\".course_abbr)::text)\n              -&gt;  Seq Scan on editions  (cost=0.00..1.65 rows=65 width=8)\n                    (actual time=0.017..0.462 rows=65 loops=1)\n              -&gt;  Hash  (cost=1.38..1.38 rows=38 width=8)\n                    (actual time=1.034..1.034 rows=0 loops=1)\n                    -&gt;  Seq Scan on courses_basetable  (cost=0.00..1.38 rows=38 width=8)\n                          (actual time=0.106..0.510 rows=38 loops=1)\n</pre>\n<p>Query 2 has more work to than Query 1.  If anyone else has other data points for this query let me know.</p>\n<p>On a side note, <a href=\"http://www.blogger.com/\">Blogger</a> really needs to support <a href=\"http://www.movabletype.org/trackback/\">TrackBack</a>, I&#8217;m tired of giving a response in a post and then having to leave a comment to let the person know where to find it.</p>\n",
            "content_text": "I&#8217;m always interested in bits about the performance of specific types of SQL queries, so I was curious when I came across Bill Schneider&#8216;s post about PostgreSQL performance of &#8220;where exists&#8221;.  He was comparing the performance of two different queries that provide the same results, so I took a look at some of my data and converted them to fit data that I already had:\nQuery 1\n\nselect course_abbr from courses_basetable\nwhere exists (\n  select 1 from editions\n  where editions.course_abbr = courses_basetable.course_abbr\n)\n\nQuery 2\n\nselect distinct(editions.course_abbr) from editions\njoin courses on\n  editions.course_abbr = courses.course_abbr\n\nBoth of these queries return the same list of course abbreviations, but perform slightly different.  To avoid the suspense I&#8217;m going to give away the ending now: Query 1 runs faster than Query 2 (tested using PostgreSQL 7.4.6), but for Bill Query 2 ran faster (using PostgreSQL 7.3.x).  For my tests Query 1 took an average time of 29 ms and Query 2 took an average of 31 ms.  So what, 2 ms you say, I throw that much CPU time away several times a minute!  The absolute times don&#8217;t really matter, the percentage difference does.  In this case Query 1 is more than 5% faster than Query 2.\nTo find out more about why these two queries perform so differently I ran them both using EXPLAIN ANALYZE to see what was going on.  It should be rather obvious why Query 2 takes longer after seeing the EXPLAIN ANALYZE results:\nQuery 1\n\nSeq Scan on courses_basetable  (cost=0.00..35.82 rows=19 width=8)\n(actual time=0.369..9.485 rows=36 loops=1)\n  Filter: (subplan)\n  SubPlan\n    -&gt;  Seq Scan on editions  (cost=0.00..1.81 rows=2 width=0)\n          (actual time=0.207..0.207 rows=1 loops=38)\n          Filter: ((course_abbr)::text = ($0)::text)\n\nQuery 2\n\nUnique  (cost=6.06..6.38 rows=36 width=8)\n(actual time=4.697..5.252 rows=36 loops=1)\n  -&gt;  Sort  (cost=6.06..6.22 rows=65 width=8)\n        (actual time=4.682..4.849 rows=65 loops=1)\n        Sort Key: editions.course_abbr\n        -&gt;  Hash Join  (cost=1.47..4.10 rows=65 width=8)\n              (actual time=1.652..3.357 rows=65 loops=1)\n              Hash Cond: ((\"outer\".course_abbr)::text = (\"inner\".course_abbr)::text)\n              -&gt;  Seq Scan on editions  (cost=0.00..1.65 rows=65 width=8)\n                    (actual time=0.017..0.462 rows=65 loops=1)\n              -&gt;  Hash  (cost=1.38..1.38 rows=38 width=8)\n                    (actual time=1.034..1.034 rows=0 loops=1)\n                    -&gt;  Seq Scan on courses_basetable  (cost=0.00..1.38 rows=38 width=8)\n                          (actual time=0.106..0.510 rows=38 loops=1)\n\nQuery 2 has more work to than Query 1.  If anyone else has other data points for this query let me know.\nOn a side note, Blogger really needs to support TrackBack, I&#8217;m tired of giving a response in a post and then having to leave a comment to let the person know where to find it.",
            "date_published": "2004-12-16T17:29:48-07:00",
            "date_modified": "2004-12-16T17:29:48-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/12/10/postgresql-vacuum/",
            "url": "https://blog.josephscott.org/2004/12/10/postgresql-vacuum/",
            "title": "PostgreSQL Vacuum",
            "content_html": "<p>Thanks to MVCC PostgreSQL does a pretty good job at avoiding locking contentions.  Unfortunately this ability does not come without a cost.  In this case the cost is having to run vacuum from time to time (the best interval for you will depend on usage) to recover rows and make that space available again.  This is especially true on a table that is changing a lot (INSERT, UPDATE and DELETE).  If you don&#8217;t do this you&#8217;ll notice things slowing down, just like <a href=\"http://www.livejournal.com/users/karl/7058.html\">Karl noticed with SELECT count(*)</a>.  Setting aside the question of doing SELECT count(*) on a table for now, if you have a table that is getting millions of changes a day then you better be looking at how often you need to run vacuum.  If it takes you years to hit a million changes then running vacuum won&#8217;t need to be as frequent.</p>\n<p>I&#8217;ve not played with pg_autovacuum yet, but that is another resource to look at for determining how often you need to vacuum your tables.</p>\n",
            "content_text": "Thanks to MVCC PostgreSQL does a pretty good job at avoiding locking contentions.  Unfortunately this ability does not come without a cost.  In this case the cost is having to run vacuum from time to time (the best interval for you will depend on usage) to recover rows and make that space available again.  This is especially true on a table that is changing a lot (INSERT, UPDATE and DELETE).  If you don&#8217;t do this you&#8217;ll notice things slowing down, just like Karl noticed with SELECT count(*).  Setting aside the question of doing SELECT count(*) on a table for now, if you have a table that is getting millions of changes a day then you better be looking at how often you need to run vacuum.  If it takes you years to hit a million changes then running vacuum won&#8217;t need to be as frequent.\nI&#8217;ve not played with pg_autovacuum yet, but that is another resource to look at for determining how often you need to vacuum your tables.",
            "date_published": "2004-12-10T11:42:34-07:00",
            "date_modified": "2004-12-10T11:42:34-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/11/26/exposing-postgresql-regular-expression-check-constraints/",
            "url": "https://blog.josephscott.org/2004/11/26/exposing-postgresql-regular-expression-check-constraints/",
            "title": "Exposing PostgreSQL Regular Expression Check Constraints",
            "content_html": "<p>Check constraints are a great feature, I was especially excited when I found out that <a href=\"http://joseph.randomnetworks.com/archives/2004/05/24/postgresql-check-constraint-supports-regular-expressions/\">PostgreSQL supports regular expressions</a> in check constraints.  Now I want to expose the regex check up to an application layer so that it can be used at input time, before the database chokes on it during an insert or an update.  So I went looking for information on how to ask PostgreSQL for the check constraint for a given table and column name.  Verlana&#8217;s <a href=\"http://www.varlena.com/varlena/GeneralBits/\">PostgreSQL General Bits</a> came to the rescue with the entry on <a href=\"http://www.varlena.com/varlena/GeneralBits/47.php\">Implementation of Constraints</a>.  The last query in the entry will lookup all on the constraints for a given table.  I modified this query to extract just the constraint regular expression for a given table and column.</p>\n<blockquote>\n<pre>\nselect substring(consrc from '^.+ [~|~*|!~|!~*] ''(.+)''.+$')\n    AS constraint_regex\nfrom pg_class r,\n    pg_constraint c\nwhere r.oid = c.conrelid\n    and contype = 'c'\n    and relname = 'your_table_name'\n    and consrc ILIKE '((your_column_name)::text %'\n</pre>\n</blockquote>\n",
            "content_text": "Check constraints are a great feature, I was especially excited when I found out that PostgreSQL supports regular expressions in check constraints.  Now I want to expose the regex check up to an application layer so that it can be used at input time, before the database chokes on it during an insert or an update.  So I went looking for information on how to ask PostgreSQL for the check constraint for a given table and column name.  Verlana&#8217;s PostgreSQL General Bits came to the rescue with the entry on Implementation of Constraints.  The last query in the entry will lookup all on the constraints for a given table.  I modified this query to extract just the constraint regular expression for a given table and column.\n\n\nselect substring(consrc from '^.+ [~|~*|!~|!~*] ''(.+)''.+$')\n    AS constraint_regex\nfrom pg_class r,\n    pg_constraint c\nwhere r.oid = c.conrelid\n    and contype = 'c'\n    and relname = 'your_table_name'\n    and consrc ILIKE '((your_column_name)::text %'",
            "date_published": "2004-11-26T16:56:23-07:00",
            "date_modified": "2004-11-26T16:56:23-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/11/15/with-features-like-this-who-needs-bugs/",
            "url": "https://blog.josephscott.org/2004/11/15/with-features-like-this-who-needs-bugs/",
            "title": "With Features Like This Who Needs Bugs",
            "content_html": "<p>When you come across things like this it is hard to resist beating them up over it one more time.  <a href=\"http://linuxintegrators.com/acoliver/\">Andrew Oliver</a> has an entry about <a href=\"http://linuxintegrators.com/acoliver/code/2004/11/20/0192.html\">dealing with BLOBs in MySQL</a>.  He points out that if your BLOB is too long &#8220;&#8230; then rather than like error or warn it just silently truncates the data&#8221;.  I&#8217;d really prefer that my database throw an error rather than alter my data.  Andrew went from Oracle to PostgreSQL to MySQL only to discover this, um, interesting feature.</p>\n<p>It should be noted that this happens when the data is inserted, which is when it should be throwing an error instead of truncating data.  This limitation is clearly spelled out in the <a href=\"http://dev.mysql.com/doc/mysql/en/BLOB.html\">BLOB documentation</a>, fifth paragraph:</p>\n<blockquote><p>\nIf you assign a value to a BLOB or TEXT column that exceeds the column type&#8217;s maximum length, the value is truncated to fit.\n</p></blockquote>\n<p>Because BLOBs can be <a href=\"http://dev.mysql.com/doc/mysql/en/Storage_requirements.html\">very large</a> it seems that you would be unlikely to run into this with LONGBLOBs.  Unfortunately that doesn&#8217;t make my gut feel any better about how wrong it is to simply truncate data that is too big when giving an error would really be the right thing to do.</p>\n",
            "content_text": "When you come across things like this it is hard to resist beating them up over it one more time.  Andrew Oliver has an entry about dealing with BLOBs in MySQL.  He points out that if your BLOB is too long &#8220;&#8230; then rather than like error or warn it just silently truncates the data&#8221;.  I&#8217;d really prefer that my database throw an error rather than alter my data.  Andrew went from Oracle to PostgreSQL to MySQL only to discover this, um, interesting feature.\nIt should be noted that this happens when the data is inserted, which is when it should be throwing an error instead of truncating data.  This limitation is clearly spelled out in the BLOB documentation, fifth paragraph:\n\nIf you assign a value to a BLOB or TEXT column that exceeds the column type&#8217;s maximum length, the value is truncated to fit.\n\nBecause BLOBs can be very large it seems that you would be unlikely to run into this with LONGBLOBs.  Unfortunately that doesn&#8217;t make my gut feel any better about how wrong it is to simply truncate data that is too big when giving an error would really be the right thing to do.",
            "date_published": "2004-11-15T19:27:28-07:00",
            "date_modified": "2004-11-15T19:27:28-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "MySQL",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/11/08/mysqls-funny-math/",
            "url": "https://blog.josephscott.org/2004/11/08/mysqls-funny-math/",
            "title": "MySQL\u2019s Funny Math",
            "content_html": "<p>Daniel Lemire came across some <a href=\"http://www.daniel-lemire.com/blog/archives/2004/11/08/funny-differences-between-mysql-and-postgresql/\">funny math in MySQL</a>.  He ran &#8216;select 11/5;&#8217; and had MySQL give him 2.20, where PostgreSQL gave him 2.  There is nothing wrong with MySQL&#8217;s answer, but it might throw off most programmers who be more likely to expect the kind of result that PostgreSQL gives.  The second query he ran was &#8216;select round(0.5);&#8217; and MySQL returns 0 and PostgreSQL returns 1.  As far as I can tell MySQL is just completely wrong here.  Those were about all the details Daniel gave, so I thought I&#8217;d try this out for myself.</p>\n<p>I was able to duplicate all of the results on MySQL 4.0.20 and PostgreSQL 7.4.3 systems.  I thought I would take this one step further and try &#8216;select 11%5;&#8217; on PostgreSQL, it gave me the correct answer of 1.  This is nicely documented under <a href=\"http://www.postgresql.org/docs/7.4/interactive/functions-math.html\">Mathematical Functions and Operators</a> in the PostgreSQL docs.  Interestingly enough MySQL also returns 1 when running &#8216;select 11%5;&#8217;.  MySQL does support integer division, but only by using the <a href=\"http://dev.mysql.com/doc/mysql/en/Arithmetic_functions.html\">DIV()</a> function.  As for rounding, the MySQL <a href=\"http://dev.mysql.com/doc/mysql/en/Mathematical_functions.html\">docs for round()</a> claim that:</p>\n<blockquote><p>\n&#8230; the behavior of ROUND() when the argument is halfway between two integers depends on the C library implementation. Different implementations round to the nearest even number, always up, always down, or always toward zero. If you need one kind of rounding, you should use a well-defined function such as TRUNCATE() or FLOOR() instead.\n</p></blockquote>\n<p>This explanation seems very odd to me since both the PostgreSQL and MySQL tests I ran were on the same system.  Perhaps PostgreSQL implements round() independently of the c libraries in order to achieve more consistent (and correct) results?  If so MySQL should do the same and not simply throw its arms up in the air if the result of round() comes out incorrectly.  If Microsoft had done something like this in Excel the screaming Slashdot hoards would be all over them.</p>\n<p>Note to MySQL developers: just fix this and move on.</p>\n",
            "content_text": "Daniel Lemire came across some funny math in MySQL.  He ran &#8216;select 11/5;&#8217; and had MySQL give him 2.20, where PostgreSQL gave him 2.  There is nothing wrong with MySQL&#8217;s answer, but it might throw off most programmers who be more likely to expect the kind of result that PostgreSQL gives.  The second query he ran was &#8216;select round(0.5);&#8217; and MySQL returns 0 and PostgreSQL returns 1.  As far as I can tell MySQL is just completely wrong here.  Those were about all the details Daniel gave, so I thought I&#8217;d try this out for myself.\nI was able to duplicate all of the results on MySQL 4.0.20 and PostgreSQL 7.4.3 systems.  I thought I would take this one step further and try &#8216;select 11%5;&#8217; on PostgreSQL, it gave me the correct answer of 1.  This is nicely documented under Mathematical Functions and Operators in the PostgreSQL docs.  Interestingly enough MySQL also returns 1 when running &#8216;select 11%5;&#8217;.  MySQL does support integer division, but only by using the DIV() function.  As for rounding, the MySQL docs for round() claim that:\n\n&#8230; the behavior of ROUND() when the argument is halfway between two integers depends on the C library implementation. Different implementations round to the nearest even number, always up, always down, or always toward zero. If you need one kind of rounding, you should use a well-defined function such as TRUNCATE() or FLOOR() instead.\n\nThis explanation seems very odd to me since both the PostgreSQL and MySQL tests I ran were on the same system.  Perhaps PostgreSQL implements round() independently of the c libraries in order to achieve more consistent (and correct) results?  If so MySQL should do the same and not simply throw its arms up in the air if the result of round() comes out incorrectly.  If Microsoft had done something like this in Excel the screaming Slashdot hoards would be all over them.\nNote to MySQL developers: just fix this and move on.",
            "date_published": "2004-11-08T19:37:47-07:00",
            "date_modified": "2004-11-08T19:37:47-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "MySQL",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/08/18/oracle-calls-it-minus-postgresql-calls-it-except/",
            "url": "https://blog.josephscott.org/2004/08/18/oracle-calls-it-minus-postgresql-calls-it-except/",
            "title": "Oracle Calls It MINUS, PostgreSQL Calls It EXCEPT",
            "content_html": "<p>Just in case I run into this again, you can use <a href=\"http://www.postgresql.org/docs/7.4/interactive/sql-select.html#SQL-EXCEPT\">EXCEPT</a> in PostgreSQL as a replacement for using <a href=\"http://hypernews.ngdc.noaa.gov/HyperNews/get/oracle/9.html?nogifs\">MINUS</a> in Oracle.  This was extremely handy this morning when I ran a query on Oracle and wanted to see if I could do the same thing on exported data in PostgreSQL.  Doing a cut-n-paste returned a syntax error because the query used MINUS.  A couple of minutes with Google and the PostgreSQL documentation and I discovered that EXCEPT does the same thing that Oracle&#8217;s MINUS does.  After changing that one word in the query it ran perfectly.</p>\n<p>Honestly, I tried to resist the urge, but I just couldn&#8217;t.  According to the MySQL <a href=\"http://dev.mysql.com/doc/mysql/en/TODO_sometime.html\">future feature list</a> MINUS, INTERSECT, and FULL OUTER JOIN will be implemented in the &#8220;mid-term future&#8221; (after 5.1).</p>\n",
            "content_text": "Just in case I run into this again, you can use EXCEPT in PostgreSQL as a replacement for using MINUS in Oracle.  This was extremely handy this morning when I ran a query on Oracle and wanted to see if I could do the same thing on exported data in PostgreSQL.  Doing a cut-n-paste returned a syntax error because the query used MINUS.  A couple of minutes with Google and the PostgreSQL documentation and I discovered that EXCEPT does the same thing that Oracle&#8217;s MINUS does.  After changing that one word in the query it ran perfectly.\nHonestly, I tried to resist the urge, but I just couldn&#8217;t.  According to the MySQL future feature list MINUS, INTERSECT, and FULL OUTER JOIN will be implemented in the &#8220;mid-term future&#8221; (after 5.1).",
            "date_published": "2004-08-18T10:19:02-06:00",
            "date_modified": "2004-08-18T10:19:02-06:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "PostgreSQL"
            ]
        },
        {
            "id": "http://joseph.randomnetworks.com/archives/2004/08/17/wordpress-using-postgresql/",
            "url": "https://blog.josephscott.org/2004/08/17/wordpress-using-postgresql/",
            "title": "WordPress Using PostgreSQL",
            "content_html": "<p>Nice to see there are other people out there who are interested in running <a href=\"http://www.wordpress.org/\">WordPress</a> on <a href=\"http://www.postgresql.org/\">PostgreSQL</a> instead of <a href=\"http://www.mysql.com/\">MySQL</a>.  It is even better to see that someone took the time to do an initial port of <a href=\"http://www.gotroot.ca/wordpress-pg/\">WordPress 1.2 to use PostgreSQL (WordPress-PG)</a>.  Here is Keenan Tims <a href=\"http://wordpress.org/pipermail/hackers_wordpress.org/2004-August/001266.html\">announcement on wp-hackers</a>.  I wonder what the best way to continue support of PostgreSQL in WordPress is as new versions are released?  There are so many things I&#8217;d like to dig in and spend time on but don&#8217;t have enough time to do them all <img src=\"https://s.w.org/images/core/emoji/13.0.1/72x72/1f641.png\" alt=\"\ud83d\ude41\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p>\n",
            "content_text": "Nice to see there are other people out there who are interested in running WordPress on PostgreSQL instead of MySQL.  It is even better to see that someone took the time to do an initial port of WordPress 1.2 to use PostgreSQL (WordPress-PG).  Here is Keenan Tims announcement on wp-hackers.  I wonder what the best way to continue support of PostgreSQL in WordPress is as new versions are released?  There are so many things I&#8217;d like to dig in and spend time on but don&#8217;t have enough time to do them all",
            "date_published": "2004-08-17T07:21:23-06:00",
            "date_modified": "2004-08-17T07:21:23-06:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "Database",
                "PostgreSQL",
                "WordPress"
            ]
        }
    ]
}