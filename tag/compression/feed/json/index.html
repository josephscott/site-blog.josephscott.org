{
    "version": "https://jsonfeed.org/version/1.1",
    "user_comment": "This feed allows you to read the posts from this site in any feed reader that supports the JSON Feed format. To add this feed to your reader, copy the following URL -- https://blog.josephscott.org/tag/compression/feed/json/ -- and add it your reader.",
    "home_page_url": "https://blog.josephscott.org/tag/compression/",
    "feed_url": "https://blog.josephscott.org/tag/compression/feed/json/",
    "language": "en-US",
    "title": "compression &#8211; Joseph Scott",
    "items": [
        {
            "id": "https://josephscott.org/?p=15634",
            "url": "https://blog.josephscott.org/2018/07/10/lossless-png-compression/",
            "title": "Lossless PNG Compression",
            "content_html": "<p>Lossless compression can be a very safe first step in reducing image sizes.  I came across this <a href=\"https://olegkikin.com/png_optimizers/\">comparison of lossless PNG compression tools</a>, and I was surprised at the range of results.  While no one single tool ranked first in all of the tests, it does show that there are some ( like <a href=\"https://github.com/google/zopfli/blob/master/README.zopflipng\">ZopfliPNG</a> ) that do a good job most of the time.</p>\n",
            "content_text": "Lossless compression can be a very safe first step in reducing image sizes.  I came across this comparison of lossless PNG compression tools, and I was surprised at the range of results.  While no one single tool ranked first in all of the tests, it does show that there are some ( like ZopfliPNG ) that do a good job most of the time.",
            "date_published": "2018-07-10T09:53:42-06:00",
            "date_modified": "2018-07-10T09:53:42-06:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "compression",
                "png",
                "Posts"
            ]
        },
        {
            "id": "https://josephscott.org/?p=14980",
            "url": "https://blog.josephscott.org/2016/01/26/brotli-coming-to-chrome/",
            "title": "Brotli Coming To Chrome",
            "content_html": "<p>I had mentioned that <a href=\"https://josephscott.org/archives/2015/12/brotli-in-browsers/\">Brotli compression was coming to Firefox</a>, but it was only listed as &#8216;in development&#8217; for Chrome.  That changed this month: <a href=\"https://groups.google.com/a/chromium.org/forum/#!searchin/blink-dev/brotli/blink-dev/JufzX024oy0/WEOGbN43AwAJ\">Intent to Ship: Brotli (Accept-encoding: br on HTTPS connection)</a>.</p>\n<p>A big issue to keep in mind: it will be limited to HTTPS connections.</p>\n<p>Perhaps this will be enough to motivate Microsoft and Apple to include support as well.</p>\n",
            "content_text": "I had mentioned that Brotli compression was coming to Firefox, but it was only listed as &#8216;in development&#8217; for Chrome.  That changed this month: Intent to Ship: Brotli (Accept-encoding: br on HTTPS connection).\nA big issue to keep in mind: it will be limited to HTTPS connections.\nPerhaps this will be enough to motivate Microsoft and Apple to include support as well.",
            "date_published": "2016-01-26T09:59:11-07:00",
            "date_modified": "2016-01-26T09:59:11-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "brotli",
                "chrome",
                "compression",
                "Posts"
            ]
        },
        {
            "id": "https://josephscott.org/?p=14602",
            "url": "https://blog.josephscott.org/2015/12/23/compressing-usertiming/",
            "title": "Compressing UserTiming",
            "content_html": "<p>Nic Jansma&#8217;s Performance Calendar post <a href=\"http://calendar.perfplanet.com/2015/compressing-usertiming/\">Compressing UserTiming</a> is a great read for those interested in collectioning UserTiming data and compression in general:</p>\n<blockquote><p>\nBasically, if you have good domain-specific knowledge of your data-structures, you can often compress better than a general-case minimizer like gzip or MessagePack.\n</p></blockquote>\n<p>Source code is available at <a href=\"https://github.com/nicjansma/usertiming-compression.js\">https://github.com/nicjansma/usertiming-compression.js</a>.</p>\n",
            "content_text": "Nic Jansma&#8217;s Performance Calendar post Compressing UserTiming is a great read for those interested in collectioning UserTiming data and compression in general:\n\nBasically, if you have good domain-specific knowledge of your data-structures, you can often compress better than a general-case minimizer like gzip or MessagePack.\n\nSource code is available at https://github.com/nicjansma/usertiming-compression.js.",
            "date_published": "2015-12-23T09:59:35-07:00",
            "date_modified": "2015-12-23T09:59:35-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "compression",
                "nic-jansma",
                "usertiming",
                "Posts"
            ]
        },
        {
            "id": "https://josephscott.org/?p=14357",
            "url": "https://blog.josephscott.org/2015/12/15/brotli-in-browsers/",
            "title": "Brotli in Browsers",
            "content_html": "<p>Last month I mentioned <a href=\"https://josephscott.org/archives/2015/11/brotli-for-nginx/\">Brotli for Nginx</a>.  On the client side, <a href=\"http://bitsup.blogspot.com/2015/09/brotli-content-encoding-for-firefox-44.html\">Firefox is aiming to add support for Brotli in version 44</a>:</p>\n<blockquote><p>\nIf all goes well in testing, Firefox 44 (ETA January 2016) will negotiate brotli as a content-encoding for https resources. The negotiation will be done in the usual way via the Accept-Encoding request header and the token &#8220;br&#8221;. Servers that wish to encode a response with brotli can do so by adding &#8220;br&#8221; to the Content-Encoding response header. Firefox won&#8217;t decode brotli outside of https &#8211; so make sure to use the HTTP content negotiation framework instead of doing user agent sniffing.\n</p></blockquote>\n<p>Chrome has <a href=\"https://www.chromestatus.com/features/5420797577396224\">Brotli support listed as &#8220;in development&#8221;</a>.  I didn&#8217;t see any indication of a current timeline for it showing up in a release version.</p>\n",
            "content_text": "Last month I mentioned Brotli for Nginx.  On the client side, Firefox is aiming to add support for Brotli in version 44:\n\nIf all goes well in testing, Firefox 44 (ETA January 2016) will negotiate brotli as a content-encoding for https resources. The negotiation will be done in the usual way via the Accept-Encoding request header and the token &#8220;br&#8221;. Servers that wish to encode a response with brotli can do so by adding &#8220;br&#8221; to the Content-Encoding response header. Firefox won&#8217;t decode brotli outside of https &#8211; so make sure to use the HTTP content negotiation framework instead of doing user agent sniffing.\n\nChrome has Brotli support listed as &#8220;in development&#8221;.  I didn&#8217;t see any indication of a current timeline for it showing up in a release version.",
            "date_published": "2015-12-15T09:38:42-07:00",
            "date_modified": "2015-12-15T09:38:42-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "brotli",
                "chrome",
                "compression",
                "firefox",
                "Posts"
            ]
        },
        {
            "id": "https://josephscott.org/?p=14048",
            "url": "https://blog.josephscott.org/2015/11/20/brotli-for-nginx/",
            "title": "Brotli for Nginx",
            "content_html": "<p>Two months ago Google announced <a href=\"http://google-opensource.blogspot.com/2015/09/introducing-brotli-new-compression.html\">Brotli</a>, a new compression format:</p>\n<blockquote><p>\nBrotli is a whole new data format. This new format allows us to get 20\u201326% higher compression ratios over Zopfli. In our study \u2018Comparison of Brotli, Deflate, Zopfli, LZMA, LZHAM and Bzip2 Compression Algorithms\u2019 we show that Brotli is roughly as fast as zlib\u2019s Deflate implementation. At the same time, it compresses slightly more densely than LZMA and bzip2 on the Canterbury corpus. The higher data density is achieved by a 2nd order context modeling, re-use of entropy codes, larger memory window of past data and joint distribution codes. Just like Zopfli, the new algorithm is named after Swiss bakery products. Br\u00f6tli means \u2018small bread\u2019 in Swiss German.\n</p></blockquote>\n<p>Compression is a big deal for web performance, being able to send the same file with fewer bytes is a big win.</p>\n<p>There are now two Nginx modules for supporting Brotli compression: <a href=\"https://github.com/google/ngx_brotli\">ngx_brotli from Google</a> and <a href=\"https://github.com/cloudflare/ngx_brotli_module\">ngx_brotli_module from CloudFlare</a>.</p>\n",
            "content_text": "Two months ago Google announced Brotli, a new compression format:\n\nBrotli is a whole new data format. This new format allows us to get 20\u201326% higher compression ratios over Zopfli. In our study \u2018Comparison of Brotli, Deflate, Zopfli, LZMA, LZHAM and Bzip2 Compression Algorithms\u2019 we show that Brotli is roughly as fast as zlib\u2019s Deflate implementation. At the same time, it compresses slightly more densely than LZMA and bzip2 on the Canterbury corpus. The higher data density is achieved by a 2nd order context modeling, re-use of entropy codes, larger memory window of past data and joint distribution codes. Just like Zopfli, the new algorithm is named after Swiss bakery products. Br\u00f6tli means \u2018small bread\u2019 in Swiss German.\n\nCompression is a big deal for web performance, being able to send the same file with fewer bytes is a big win.\nThere are now two Nginx modules for supporting Brotli compression: ngx_brotli from Google and ngx_brotli_module from CloudFlare.",
            "date_published": "2015-11-20T09:15:55-07:00",
            "date_modified": "2015-11-20T09:15:55-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "brotli",
                "cloudflare",
                "compression",
                "google",
                "nginx",
                "Posts"
            ]
        },
        {
            "id": "https://josephscott.org/?p=6918",
            "url": "https://blog.josephscott.org/2013/03/06/zopfli-compression/",
            "title": "Zopfli Compression",
            "content_html": "<figure style=\"width: 128px\" class=\"wp-caption alignleft\"><img loading=\"lazy\" alt=\"Zopfli was named after a Swiss bread recipe\" src=\"http://2.bp.blogspot.com/-jDq-ZRd-xlQ/US_LwJaVR4I/AAAAAAAACh0/9T2OTpzRj5E/s1600/zopfli.jpg\" width=\"128\" height=\"108\" /><figcaption class=\"wp-caption-text\">Zopfli was named after a Swiss bread recipe</figcaption></figure>\n<p>Google recently <a href=\"http://googledevelopers.blogspot.com/2013/02/compress-data-more-densely-with-zopfli.html\">announced</a> the new <a href=\"https://code.google.com/p/zopfli/\">Zopfli Compression Algorithm</a>:</p>\n<blockquote><p>Zopfli Compression Algorithm is a new zlib (gzip, deflate) compatible compressor. This compressor takes more time (~100x slower), but compresses around 5% better than zlib and better than any other zlib-compatible compressor we have found.</p></blockquote>\n<p>Being gzip compatible means that existing clients can decompress files that have been compressed with Zopfli. Specifically, web browsers will be able to understand this.</p>\n<p>A number of people have asked what the big deal is over such a relatively small size reduction at the cost of a much slower compression process. Lets take the lastest <a href=\"https://developers.google.com/speed/libraries/devguide#jquery\">jQuery file from the Google CDN</a> as an example &#8211; <a href=\"http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js\">http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js</a>.</p>\n<p>First I cloned the <a href=\"https://code.google.com/p/zopfli/source/checkout\">Zopfli source repo</a> and compiled it on my Macbook Air. Took about a minute or so to clone and build. The build process only required running <code>make</code>.</p>\n<p>Here are the results of <code>gzip -9</code> and <code>zopfli &#8211; i1000</code> on jquery.min.js:</p>\n<table>\n<thead>\n<tr>\n<th>Compression</th>\n<th>Size</th>\n<th>Compression Time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>None</td>\n<td>92,629 bytes</td>\n<td>&#8211;</td>\n</tr>\n<tr>\n<td>gzip -9</td>\n<td>32,660 bytes</td>\n<td>0.009s</td>\n</tr>\n<tr>\n<td>zopfli &#8211;i1000</td>\n<td>31,686 bytes</td>\n<td>16.376s</td>\n</tr>\n</tbody>\n</table>\n<p>In this test Zopfli saved an additional 974 bytes and took over 16 seconds longer. In our Google CDN example the time it takes to do the compression doesn&#8217;t make any difference, that is something you&#8217;ll only be doing once per file. To see what the potential savings is from those extra 974 bytes we&#8217;d need to know how often the jQuery file is downloaded. I don&#8217;t know what the actual numbers are, so lets make some up. Lets say it is 1 million times per month.</p>\n<p>So 974 bytes * 1,000,000 gives us 974,000,000 bytes per month in savings. No doubt this is a drop in the bucket when compared to total bandwidth usage at Google, but it is an improvement. The improvement wouldn&#8217;t just be for Google either, everyone who views web sites that had the Zopfli compressed jQuery would have a slightly better experience as well. Smaller file means it gets downloaded faster over my existing connection. This would be extra good for mobile users.</p>\n<p>I think there are plenty of cases where using Zopfli to compress your files will be a nice little improvement.</p>\n",
            "content_text": "Zopfli was named after a Swiss bread recipe\nGoogle recently announced the new Zopfli Compression Algorithm:\nZopfli Compression Algorithm is a new zlib (gzip, deflate) compatible compressor. This compressor takes more time (~100x slower), but compresses around 5% better than zlib and better than any other zlib-compatible compressor we have found.\nBeing gzip compatible means that existing clients can decompress files that have been compressed with Zopfli. Specifically, web browsers will be able to understand this.\nA number of people have asked what the big deal is over such a relatively small size reduction at the cost of a much slower compression process. Lets take the lastest jQuery file from the Google CDN as an example &#8211; http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js.\nFirst I cloned the Zopfli source repo and compiled it on my Macbook Air. Took about a minute or so to clone and build. The build process only required running make.\nHere are the results of gzip -9 and zopfli &#8211; i1000 on jquery.min.js:\n\n\n\nCompression\nSize\nCompression Time\n\n\n\n\nNone\n92,629 bytes\n&#8211;\n\n\ngzip -9\n32,660 bytes\n0.009s\n\n\nzopfli &#8211;i1000\n31,686 bytes\n16.376s\n\n\n\nIn this test Zopfli saved an additional 974 bytes and took over 16 seconds longer. In our Google CDN example the time it takes to do the compression doesn&#8217;t make any difference, that is something you&#8217;ll only be doing once per file. To see what the potential savings is from those extra 974 bytes we&#8217;d need to know how often the jQuery file is downloaded. I don&#8217;t know what the actual numbers are, so lets make some up. Lets say it is 1 million times per month.\nSo 974 bytes * 1,000,000 gives us 974,000,000 bytes per month in savings. No doubt this is a drop in the bucket when compared to total bandwidth usage at Google, but it is an improvement. The improvement wouldn&#8217;t just be for Google either, everyone who views web sites that had the Zopfli compressed jQuery would have a slightly better experience as well. Smaller file means it gets downloaded faster over my existing connection. This would be extra good for mobile users.\nI think there are plenty of cases where using Zopfli to compress your files will be a nice little improvement.",
            "date_published": "2013-03-06T13:05:53-07:00",
            "date_modified": "2013-03-06T13:05:53-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "compression",
                "google",
                "zlib",
                "zopfli",
                "Posts"
            ]
        },
        {
            "id": "http://josephscott.org/?p=5118",
            "url": "https://blog.josephscott.org/2011/11/18/googles-plusone-js-doesnt-support-http-compression/",
            "title": "Google\u2019s plusone.js Doesn\u2019t Support HTTP Compression",
            "content_html": "<p>I was surprised to see that <a href=\"https://developers.google.com/+/plugins/+1button/\">Google&#8217;s plusone.js</a> doesn&#8217;t support HTTP compression.  Here is a quick test with<br />\n<code>curl -v &#8211; compressed https://apis.google.com/js/plusone.js > /dev/null</code></p>\n<p><strong>Request Headers:</strong></p>\n<pre>\n> GET /js/plusone.js HTTP/1.1\n> User-Agent: curl/7.19.7 (universal-apple-darwin10.0) libcurl/7.19.7 OpenSSL/0.9.8r zlib/1.2.3\n> Host: apis.google.com\n> Accept: */*\n> Accept-Encoding: deflate, gzip\n</pre>\n<p><strong>Response Headers:</strong></p>\n<pre>\nHTTP/1.1 200 OK\n< Content-Type: text/javascript; charset=utf-8\n< Expires: Fri, 18 Nov 2011 02:35:20 GMT\n< Date: Fri, 18 Nov 2011 02:35:20 GMT\n< Cache-Control: private, max-age=3600\n< X-Content-Type-Options: nosniff\n< X-Frame-Options: SAMEORIGIN\n< X-XSS-Protection: 1; mode=block\n< Server: GSE\n< Transfer-Encoding: chunked\n</pre>\n<p>You'll notice there is no <code>Content-Encoding: gzip</code> header in the response.</p>\n<p>We'll have to get <a href=\"http://stevesouders.com/\">Steve Souders</a> to pester them about that.</p>\n",
            "content_text": "I was surprised to see that Google&#8217;s plusone.js doesn&#8217;t support HTTP compression.  Here is a quick test with\ncurl -v &#8211; compressed https://apis.google.com/js/plusone.js > /dev/null\nRequest Headers:\n\n> GET /js/plusone.js HTTP/1.1\n> User-Agent: curl/7.19.7 (universal-apple-darwin10.0) libcurl/7.19.7 OpenSSL/0.9.8r zlib/1.2.3\n> Host: apis.google.com\n> Accept: */*\n> Accept-Encoding: deflate, gzip\n\nResponse Headers:\n\nHTTP/1.1 200 OK\n< Content-Type: text/javascript; charset=utf-8\n< Expires: Fri, 18 Nov 2011 02:35:20 GMT\n< Date: Fri, 18 Nov 2011 02:35:20 GMT\n< Cache-Control: private, max-age=3600\n< X-Content-Type-Options: nosniff\n< X-Frame-Options: SAMEORIGIN\n< X-XSS-Protection: 1; mode=block\n< Server: GSE\n< Transfer-Encoding: chunked\n\nYou'll notice there is no Content-Encoding: gzip header in the response.\nWe'll have to get Steve Souders to pester them about that.",
            "date_published": "2011-11-18T10:51:47-07:00",
            "date_modified": "2011-11-18T10:51:47-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "compression",
                "google",
                "http",
                "javascript",
                "performance",
                "Posts"
            ]
        },
        {
            "id": "http://josephscott.org/?p=3167",
            "url": "https://blog.josephscott.org/2010/11/12/gzip-support-for-amazon-web-services-cloudfront/",
            "title": "gzip support for Amazon Web Services CloudFront",
            "content_html": "<blockquote><p>With the recent announcement of Custom Origin support in CloudFront it is now possible to use the standard HTTP Accept-Encoding method for serving gzipped content if you are using a Custom Origin. Although not specifically mentioned in the release announcement you can verify this in the Custom Origins Appendix of the CloudFront Developer Guide. CloudFront will now forward the Accept-Encoding HTTP header to your origin server where you can ensure the appropriate content is served based on the supported encodings. CloudFront will then cache multiple versions of this content, the uncompressed version and the gzipped version and serve these to clients depending on the value of their Accept-Encoding header for all future requests.</p></blockquote>\n<p>via <a href='http://www.nomitor.com/blog/2010/11/10/gzip-support-for-amazon-web-services-cloudfront/'>gzip support for Amazon Web Services CloudFront &#8211; nomitor</a>.</p>\n",
            "content_text": "With the recent announcement of Custom Origin support in CloudFront it is now possible to use the standard HTTP Accept-Encoding method for serving gzipped content if you are using a Custom Origin. Although not specifically mentioned in the release announcement you can verify this in the Custom Origins Appendix of the CloudFront Developer Guide. CloudFront will now forward the Accept-Encoding HTTP header to your origin server where you can ensure the appropriate content is served based on the supported encodings. CloudFront will then cache multiple versions of this content, the uncompressed version and the gzipped version and serve these to clients depending on the value of their Accept-Encoding header for all future requests.\nvia gzip support for Amazon Web Services CloudFront &#8211; nomitor.",
            "date_published": "2010-11-12T10:11:11-07:00",
            "date_modified": "2010-11-12T10:11:11-07:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "amazon",
                "cloudfront",
                "compression",
                "http",
                "Asides"
            ]
        },
        {
            "id": "http://josephscott.org/?p=3081",
            "url": "https://blog.josephscott.org/2010/11/01/user-agent-sniffing-at-google-libraries-cdn/",
            "title": "User Agent Sniffing at Google Libraries CDN",
            "content_html": "<p>I recently took a closer look at <a href=\"http://code.google.com/apis/libraries/\">Google Libraries</a>, their content delivery network (CDN) for various Javascript libraries, and HTTP compression.  I started with a simple test:</p>\n<p>[sourcecode syn=&#8221;plain&#8221;]<br />\ncurl -O -v &#8211;compressed http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js<br />\n[/sourcecode]</p>\n<p>This downloads a minified version of jQuery 1.4.3, with <code>--compressed</code>, which means I&#8217;d like the response to be compressed.  The HTTP request looked like:</p>\n<p>[sourcecode syn=&#8221;plain&#8221;]<br />\n&gt; GET /ajax/libs/jquery/1.4.3/jquery.min.js HTTP/1.1<br />\n&gt; User-Agent: curl/7.16.4 (i386-apple-darwin9.0) libcurl/7.16.4 OpenSSL/0.9.7l zlib/1.2.3<br />\n&gt; Host: ajax.googleapis.com<br />\n&gt; Accept: */*<br />\n&gt; Accept-Encoding: deflate, gzip<br />\n&gt;<br />\n[/sourcecode]</p>\n<p>The  response from Google was:</p>\n<p>[sourcecode syn=&#8221;plain&#8221;]<br />\n&lt; HTTP/1.1 200 OK<br />\n&lt; Content-Type: text/javascript; charset=UTF-8<br />\n&lt; Last-Modified: Fri, 15 Oct 2010 18:25:24 GMT<br />\n&lt; Date: Fri, 29 Oct 2010 03:27:16 GMT<br />\n&lt; Expires: Sat, 29 Oct 2011 03:27:16 GMT<br />\n&lt; Vary: Accept-Encoding<br />\n&lt; X-Content-Type-Options: nosniff<br />\n&lt; Server: sffe<br />\n&lt; Cache-Control: public, max-age=31536000<br />\n&lt; Age: 145355<br />\n&lt; Transfer-Encoding: chunked<br />\n&lt;<br />\n[/sourcecode]</p>\n<p>I was surprised that there was no <code>Content-Encoding: gzip</code> header in the response, meaning the response was NOT compressed.  I wasn&#8217;t quite sure what to make of this at first.  No way would Google forget to turn on HTTP compression, I must have missed something.  I stared at the HTTP response for sometime, trying to figure out what I was missing.  Nothing came to mind, so  I ran another test.</p>\n<p>This time I made a request for http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js in Firefox 3.6.12 on Mac OS X and used Firebug to inspect the HTTP transaction.  The request:</p>\n<p>[sourcecode lang=&#8221;plain&#8221;]<br />\nGET /ajax/libs/jquery/1.4.3/jquery.min.js HTTP/1.1<br />\nHost: ajax.googleapis.com<br />\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12<br />\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8<br />\nAccept-Language: en-us,en;q=0.5<br />\nAccept-Encoding: gzip,deflate<br />\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7<br />\nKeep-Alive: 115<br />\nConnection: keep-alive<br />\nPragma: no-cache<br />\nCache-Control: no-cache<br />\n[/sourcecode]</p>\n<p>and the response:</p>\n<p>[sourcecode lang=&#8221;plain&#8221;]<br />\nHTTP/1.1 200 OK<br />\nContent-Type: text/javascript; charset=UTF-8<br />\nLast-Modified: Fri, 15 Oct 2010 18:25:24 GMT<br />\nDate: Fri, 29 Oct 2010 03:12:35 GMT<br />\nExpires: Sat, 29 Oct 2011 03:12:35 GMT<br />\nVary: Accept-Encoding<br />\nX-Content-Type-Options: nosniff<br />\nServer: sffe<br />\nContent-Encoding: gzip<br />\nCache-Control: public, max-age=31536000<br />\nContent-Length: 26769<br />\nAge: 147128<br />\n[/sourcecode]</p>\n<p>This time the content was compressed.  There were several differences in the request headers between curl and Firefox, I decided to start with just one, the &#8220;User-Agent&#8221;.  I modified my initial curl request to include the User-Agent string from Firefox:</p>\n<p>[sourcecode syn=&#8221;plain&#8221;]<br />\ncurl -O -v &#8211;compressed &#8211;user-agent &quot;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12&quot; http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js<br />\n[/sourcecode]</p>\n<p>The request:</p>\n<p>[sourcecode syn=&#8221;plain&#8221;]<br />\n&gt; GET /ajax/libs/jquery/1.4.3/jquery.min.js HTTP/1.1<br />\n&gt; User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12<br />\n&gt; Host: ajax.googleapis.com<br />\n&gt; Accept: */*<br />\n&gt; Accept-Encoding: deflate, gzip<br />\n&gt;<br />\n[/sourcecode]</p>\n<p>and the response:</p>\n<p>[sourcecode syn=&#8221;plain&#8221;]<br />\n&lt; HTTP/1.1 200 OK<br />\n&lt; Content-Type: text/javascript; charset=UTF-8<br />\n&lt; Last-Modified: Fri, 15 Oct 2010 18:25:24 GMT<br />\n&lt; Date: Fri, 29 Oct 2010 03:33:09 GMT<br />\n&lt; Expires: Sat, 29 Oct 2011 03:33:09 GMT<br />\n&lt; Vary: Accept-Encoding<br />\n&lt; X-Content-Type-Options: nosniff<br />\n&lt; Server: sffe<br />\n&lt; Content-Encoding: gzip<br />\n&lt; Cache-Control: public, max-age=31536000<br />\n&lt; Content-Length: 26769<br />\n&lt; Age: 147018<br />\n&lt;<br />\n[/sourcecode]</p>\n<p>Sure enough, I got back a compressed response.  Google was sniffing the User-Agent string to determine if a compressed response should be sent.  It didn&#8217;t matter if the client asked for a compressed response ( <code>Accept-Encoding: deflate, gzip</code>) or not.  What still wasn&#8217;t clear is if this was a black list approach (singling out curl) or a white list approach (Firefox is okay).  So I tried a few other requests with various User-Agent strings.  First up, no User-Agent set at all:</p>\n<p>[sourcecode syn=&#8221;plain&#8221;]<br />\ncurl -O -v &#8211;compressed &#8211;user-agent &quot;&quot; http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js<br />\n[/sourcecode]</p>\n<p>Not compressed.  Next a made up string:</p>\n<p>[sourcecode syn=&#8221;plain&#8221;]<br />\ncurl -O -v &#8211;compressed &#8211;user-agent &quot;JosephScott/1.0 test/2.0&quot; http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js<br />\n[/sourcecode]</p>\n<p>Not compressed.  At this point I think Google is using a white list approach, if you aren&#8217;t on the list of approved User-Agent strings for getting a compressed response then you won&#8217;t get one, no matter how nicely you ask.</p>\n<p>I collected a few more browser samples as well, just to be sure:</p>\n<ul>\n<li>Safari 5.0.2 on Mac OS X &#8211; compressed</li>\n<li>IE 8 on Windows XP &#8211; compressed</li>\n<li>Firefox 3.6.12 on Windows XP &#8211; compressed</li>\n<li>Chrome 7.0.517.41 beta on Windows XP &#8211; compressed</li>\n<li>Opera 10.63 on Windows XP &#8211; NOT compressed</li>\n<li>Safari 5.0.2 on Windows XP &#8211; compressed</li>\n</ul>\n<p>One more time, curl using the IE 8 User-Agent string:</p>\n<p>[sourcecode syn=&#8221;plain&#8221;]<br />\ncurl -O -v &#8211;compressed &#8211;user-agent &quot;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET4.0C;&quot; http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js<br />\n[/sourcecode]</p>\n<p>Compressed.</p>\n<p>Since I can manipulate the response based on the User-Agent value I&#8217;m left to conclude that the Google Library CDN sniffs the User-Agent string to determine if it will respond with a compressed result.  From what I&#8217;ve seen so far Google Library contains a white list of approved User-Agent patterns that it checks against to determine if it will honor the compression request.</p>\n<p>If you are on a current version of one of the popular browsers you will get a compressed response.  For those using anything else you&#8217;ll have to test to confirm if Google Library will honor your request for compressed content.  Opera users are just plain out of luck, even the most recent version gets an uncompressed response.</p>\n",
            "content_text": "I recently took a closer look at Google Libraries, their content delivery network (CDN) for various Javascript libraries, and HTTP compression.  I started with a simple test:\n[sourcecode syn=&#8221;plain&#8221;]\ncurl -O -v &#8211;compressed http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js\n[/sourcecode]\nThis downloads a minified version of jQuery 1.4.3, with --compressed, which means I&#8217;d like the response to be compressed.  The HTTP request looked like:\n[sourcecode syn=&#8221;plain&#8221;]\n&gt; GET /ajax/libs/jquery/1.4.3/jquery.min.js HTTP/1.1\n&gt; User-Agent: curl/7.16.4 (i386-apple-darwin9.0) libcurl/7.16.4 OpenSSL/0.9.7l zlib/1.2.3\n&gt; Host: ajax.googleapis.com\n&gt; Accept: */*\n&gt; Accept-Encoding: deflate, gzip\n&gt;\n[/sourcecode]\nThe  response from Google was:\n[sourcecode syn=&#8221;plain&#8221;]\n&lt; HTTP/1.1 200 OK\n&lt; Content-Type: text/javascript; charset=UTF-8\n&lt; Last-Modified: Fri, 15 Oct 2010 18:25:24 GMT\n&lt; Date: Fri, 29 Oct 2010 03:27:16 GMT\n&lt; Expires: Sat, 29 Oct 2011 03:27:16 GMT\n&lt; Vary: Accept-Encoding\n&lt; X-Content-Type-Options: nosniff\n&lt; Server: sffe\n&lt; Cache-Control: public, max-age=31536000\n&lt; Age: 145355\n&lt; Transfer-Encoding: chunked\n&lt;\n[/sourcecode]\nI was surprised that there was no Content-Encoding: gzip header in the response, meaning the response was NOT compressed.  I wasn&#8217;t quite sure what to make of this at first.  No way would Google forget to turn on HTTP compression, I must have missed something.  I stared at the HTTP response for sometime, trying to figure out what I was missing.  Nothing came to mind, so  I ran another test.\nThis time I made a request for http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js in Firefox 3.6.12 on Mac OS X and used Firebug to inspect the HTTP transaction.  The request:\n[sourcecode lang=&#8221;plain&#8221;]\nGET /ajax/libs/jquery/1.4.3/jquery.min.js HTTP/1.1\nHost: ajax.googleapis.com\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-us,en;q=0.5\nAccept-Encoding: gzip,deflate\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\nKeep-Alive: 115\nConnection: keep-alive\nPragma: no-cache\nCache-Control: no-cache\n[/sourcecode]\nand the response:\n[sourcecode lang=&#8221;plain&#8221;]\nHTTP/1.1 200 OK\nContent-Type: text/javascript; charset=UTF-8\nLast-Modified: Fri, 15 Oct 2010 18:25:24 GMT\nDate: Fri, 29 Oct 2010 03:12:35 GMT\nExpires: Sat, 29 Oct 2011 03:12:35 GMT\nVary: Accept-Encoding\nX-Content-Type-Options: nosniff\nServer: sffe\nContent-Encoding: gzip\nCache-Control: public, max-age=31536000\nContent-Length: 26769\nAge: 147128\n[/sourcecode]\nThis time the content was compressed.  There were several differences in the request headers between curl and Firefox, I decided to start with just one, the &#8220;User-Agent&#8221;.  I modified my initial curl request to include the User-Agent string from Firefox:\n[sourcecode syn=&#8221;plain&#8221;]\ncurl -O -v &#8211;compressed &#8211;user-agent &quot;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12&quot; http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js\n[/sourcecode]\nThe request:\n[sourcecode syn=&#8221;plain&#8221;]\n&gt; GET /ajax/libs/jquery/1.4.3/jquery.min.js HTTP/1.1\n&gt; User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12\n&gt; Host: ajax.googleapis.com\n&gt; Accept: */*\n&gt; Accept-Encoding: deflate, gzip\n&gt;\n[/sourcecode]\nand the response:\n[sourcecode syn=&#8221;plain&#8221;]\n&lt; HTTP/1.1 200 OK\n&lt; Content-Type: text/javascript; charset=UTF-8\n&lt; Last-Modified: Fri, 15 Oct 2010 18:25:24 GMT\n&lt; Date: Fri, 29 Oct 2010 03:33:09 GMT\n&lt; Expires: Sat, 29 Oct 2011 03:33:09 GMT\n&lt; Vary: Accept-Encoding\n&lt; X-Content-Type-Options: nosniff\n&lt; Server: sffe\n&lt; Content-Encoding: gzip\n&lt; Cache-Control: public, max-age=31536000\n&lt; Content-Length: 26769\n&lt; Age: 147018\n&lt;\n[/sourcecode]\nSure enough, I got back a compressed response.  Google was sniffing the User-Agent string to determine if a compressed response should be sent.  It didn&#8217;t matter if the client asked for a compressed response ( Accept-Encoding: deflate, gzip) or not.  What still wasn&#8217;t clear is if this was a black list approach (singling out curl) or a white list approach (Firefox is okay).  So I tried a few other requests with various User-Agent strings.  First up, no User-Agent set at all:\n[sourcecode syn=&#8221;plain&#8221;]\ncurl -O -v &#8211;compressed &#8211;user-agent &quot;&quot; http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js\n[/sourcecode]\nNot compressed.  Next a made up string:\n[sourcecode syn=&#8221;plain&#8221;]\ncurl -O -v &#8211;compressed &#8211;user-agent &quot;JosephScott/1.0 test/2.0&quot; http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js\n[/sourcecode]\nNot compressed.  At this point I think Google is using a white list approach, if you aren&#8217;t on the list of approved User-Agent strings for getting a compressed response then you won&#8217;t get one, no matter how nicely you ask.\nI collected a few more browser samples as well, just to be sure:\n\nSafari 5.0.2 on Mac OS X &#8211; compressed\nIE 8 on Windows XP &#8211; compressed\nFirefox 3.6.12 on Windows XP &#8211; compressed\nChrome 7.0.517.41 beta on Windows XP &#8211; compressed\nOpera 10.63 on Windows XP &#8211; NOT compressed\nSafari 5.0.2 on Windows XP &#8211; compressed\n\nOne more time, curl using the IE 8 User-Agent string:\n[sourcecode syn=&#8221;plain&#8221;]\ncurl -O -v &#8211;compressed &#8211;user-agent &quot;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET4.0C;&quot; http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js\n[/sourcecode]\nCompressed.\nSince I can manipulate the response based on the User-Agent value I&#8217;m left to conclude that the Google Library CDN sniffs the User-Agent string to determine if it will respond with a compressed result.  From what I&#8217;ve seen so far Google Library contains a white list of approved User-Agent patterns that it checks against to determine if it will honor the compression request.\nIf you are on a current version of one of the popular browsers you will get a compressed response.  For those using anything else you&#8217;ll have to test to confirm if Google Library will honor your request for compressed content.  Opera users are just plain out of luck, even the most recent version gets an uncompressed response.",
            "date_published": "2010-11-01T09:06:26-06:00",
            "date_modified": "2010-11-01T09:06:26-06:00",
            "authors": [
                {
                    "name": "josephscott",
                    "url": "https://blog.josephscott.org/author/josephscott/",
                    "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
                }
            ],
            "author": {
                "name": "josephscott",
                "url": "https://blog.josephscott.org/author/josephscott/",
                "avatar": "https://secure.gravatar.com/avatar/582b66ad5ae1b69c7601a990cb9a661a?s=512&d=mm&r=g"
            },
            "tags": [
                "cdn",
                "compression",
                "google",
                "http",
                "javascript",
                "user-agent",
                "Posts"
            ]
        }
    ]
}